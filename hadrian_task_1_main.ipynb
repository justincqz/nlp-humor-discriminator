{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Hadrian's second copy of task_1_main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8762ac9342f44c26b866e7a8ad174594": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bb9dead128774fdfb6e910ff3dfa8324",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b69e8a70ff1c41fbaf623892e0efc39b",
              "IPY_MODEL_ce41a07683fb4e5a93d21b5208ee2e2b"
            ]
          }
        },
        "bb9dead128774fdfb6e910ff3dfa8324": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b69e8a70ff1c41fbaf623892e0efc39b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6b745844d2fc4458a17500f37e20876f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898823,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898823,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f17e3e7121d443ae88bbbec6f2f417d7"
          }
        },
        "ce41a07683fb4e5a93d21b5208ee2e2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0ef8dcd1f2194af8b8989b4d1dd27fe3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 899k/899k [00:00&lt;00:00, 8.33MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ba90843ee83a4711817a25af21b73e11"
          }
        },
        "6b745844d2fc4458a17500f37e20876f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f17e3e7121d443ae88bbbec6f2f417d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0ef8dcd1f2194af8b8989b4d1dd27fe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ba90843ee83a4711817a25af21b73e11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c3f8e38c826749d0ab821e519773d3e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1e42b02f8951449eb3dd0cb7e193be57",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6be2fd8ccb114e2f9e701d0053f50a9a",
              "IPY_MODEL_1a7f3085d61144e6828f40f9bde4b321"
            ]
          }
        },
        "1e42b02f8951449eb3dd0cb7e193be57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6be2fd8ccb114e2f9e701d0053f50a9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0b55f05ca46b49c4832efa86512afb5d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_43768c2dc1dd4609896bd34d1757a005"
          }
        },
        "1a7f3085d61144e6828f40f9bde4b321": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7f4616f9a80e4a26a1b1266c84c5776d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 4.74MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f1b001d914d845b0b6d47137b5c42a38"
          }
        },
        "0b55f05ca46b49c4832efa86512afb5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "43768c2dc1dd4609896bd34d1757a005": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7f4616f9a80e4a26a1b1266c84c5776d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f1b001d914d845b0b6d47137b5c42a38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "741fd16424c54f8b89324c8b2058daf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b5793bcfe41d48198fbabeaea96b38a8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f450c9f588b24c788630c7678d664fef",
              "IPY_MODEL_5892e97cab574a729e9597b7a2b67b47"
            ]
          }
        },
        "b5793bcfe41d48198fbabeaea96b38a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f450c9f588b24c788630c7678d664fef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_af175e27d00d4a648327ee404a50249a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 481,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 481,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c939ae3ce69c44abae1b659222fa382f"
          }
        },
        "5892e97cab574a729e9597b7a2b67b47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f7513219a2cc4100a30958750bbac674",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 481/481 [00:00&lt;00:00, 2.70kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bd8b56328ad747d0afa8e62139778373"
          }
        },
        "af175e27d00d4a648327ee404a50249a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c939ae3ce69c44abae1b659222fa382f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f7513219a2cc4100a30958750bbac674": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bd8b56328ad747d0afa8e62139778373": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fd7e20a0c1a14ff0980544b3e28da5c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c4bcc7fd11c84289b4e35739cfd239cf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e73980c9a63b4bacbea15dad1d7f471b",
              "IPY_MODEL_12b34fec2fc34a65ab7a7b5f2176b7fc"
            ]
          }
        },
        "c4bcc7fd11c84289b4e35739cfd239cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e73980c9a63b4bacbea15dad1d7f471b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_de49df14f18b446eb84e6b3a428b5d15",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 501200538,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 501200538,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4f1f45255acf490988852ab8aaf42290"
          }
        },
        "12b34fec2fc34a65ab7a7b5f2176b7fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3ba08cd0cf964391a1a6485de6946f8a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 501M/501M [00:06&lt;00:00, 72.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3f2fe1c785d9448ca603771dd8a63e4a"
          }
        },
        "de49df14f18b446eb84e6b3a428b5d15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4f1f45255acf490988852ab8aaf42290": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3ba08cd0cf964391a1a6485de6946f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3f2fe1c785d9448ca603771dd8a63e4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWRt9UuE3BQP"
      },
      "source": [
        "### Coursework coding instructions (please also see full coursework spec)\n",
        "\n",
        "Please choose if you want to do either Task 1 or Task 2. You should write your report about one task only.\n",
        "\n",
        "For the task you choose you will need to do two approaches:\n",
        "  - Approach 1, which can use use pre-trained embeddings / models\n",
        "  - Approach 2, which should not use any pre-trained embeddings or models\n",
        "We should be able to run both approaches from the same colab file\n",
        "\n",
        "#### Running your code:\n",
        "  - Your models should run automatically when running your colab file without further intervention\n",
        "  - For each task you should automatically output the performance of both models\n",
        "  - Your code should automatically download any libraries required\n",
        "\n",
        "#### Structure of your code:\n",
        "  - You are expected to use the 'train', 'eval' and 'model_performance' functions, although you may edit these as required\n",
        "  - Otherwise there are no restrictions on what you can do in your code\n",
        "\n",
        "#### Documentation:\n",
        "  - You are expected to produce a .README file summarising how you have approached both tasks\n",
        "\n",
        "#### Reproducibility:\n",
        "  - Your .README file should explain how to replicate the different experiments mentioned in your report\n",
        "\n",
        "Good luck! We are really looking forward to seeing your reports and your model code!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQN7xeP23BQW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46e97636-4faa-4787-a2e0-23c548c4b99b"
      },
      "source": [
        "# You will need to download any word embeddings required for your code, e.g.:\n",
        "\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "# For any packages that Colab does not provide auotmatically you will also need to install these below, e.g.:\n",
        "\n",
        "#! pip install torch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-13 12:12:44--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-02-13 12:12:44--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-02-13 12:12:45--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: â€˜glove.6B.zipâ€™\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.02MB/s    in 6m 51s  \n",
            "\n",
            "2021-02-13 12:19:36 (2.00 MB/s) - â€˜glove.6B.zipâ€™ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL0-upkD3BQY"
      },
      "source": [
        "# Imports\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import codecs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71X4eUyQudvc",
        "outputId": "ccf72f7c-6e41-4fd4-90d3-44566319aa36"
      },
      "source": [
        "!pip install transformers\r\n",
        "!pip install torch\r\n",
        "!pip install skorch\r\n",
        "\r\n",
        "import torch\r\n",
        "from transformers import BertTokenizer, BertModel, RobertaTokenizer, RobertaModel"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/ef312eef26f5cecd8b17ae9654cdd8d1fae1eb6dbd87257d6d73c128a4d0/transformers-4.3.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.8MB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 47.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.2MB 50.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=65feaa8b8c1cb42327c2220ef2eb6248130bf5fe28f6e8067cb9cec18b7af0a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.2\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.19.5)\n",
            "Collecting skorch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/c7/2f6434f9360c91a4bf14ae85f634758e5dacd3539cca4266a60be9f881ae/skorch-0.9.0-py3-none-any.whl (125kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133kB 8.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.22.2.post1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.8.7)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (4.41.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from skorch) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->skorch) (1.0.0)\n",
            "Installing collected packages: skorch\n",
            "Successfully installed skorch-0.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ov0JGF5J3BQZ"
      },
      "source": [
        "# Setting random seed and device\n",
        "SEED = 1\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oZQFbuF3BQa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69155433-93eb-407e-f73b-24ba82f15180"
      },
      "source": [
        "# Load data\n",
        "\n",
        "!wget -O train.csv https://drive.google.com/u/0/uc?id=1UgrdjcHHZmAthjusQDAKoSqd37up-41f&export=download\n",
        "!wget -O dev.csv https://drive.google.com/u/0/uc?id=1rY6A0cN_cxAMK3aMHlTFWxhbcLFomvQL&export=download\n",
        "\n",
        "train_df = pd.read_csv('./train.csv')\n",
        "test_df = pd.read_csv('./dev.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-20 09:02:30--  https://drive.google.com/u/0/uc?id=1UgrdjcHHZmAthjusQDAKoSqd37up-41f\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.197.102, 74.125.197.139, 74.125.197.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.197.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-00-cc-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/32agauhpgmd4ls7mvf9p51jtmooi5cvj/1613811750000/13802342090854404605/*/1UgrdjcHHZmAthjusQDAKoSqd37up-41f [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-02-20 09:02:31--  https://doc-00-cc-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/32agauhpgmd4ls7mvf9p51jtmooi5cvj/1613811750000/13802342090854404605/*/1UgrdjcHHZmAthjusQDAKoSqd37up-41f\n",
            "Resolving doc-00-cc-docs.googleusercontent.com (doc-00-cc-docs.googleusercontent.com)... 74.125.195.132, 2607:f8b0:400e:c09::84\n",
            "Connecting to doc-00-cc-docs.googleusercontent.com (doc-00-cc-docs.googleusercontent.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 947914 (926K) [text/csv]\n",
            "Saving to: â€˜train.csvâ€™\n",
            "\n",
            "train.csv           100%[===================>] 925.70K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2021-02-20 09:02:32 (162 MB/s) - â€˜train.csvâ€™ saved [947914/947914]\n",
            "\n",
            "--2021-02-20 09:02:32--  https://drive.google.com/u/0/uc?id=1rY6A0cN_cxAMK3aMHlTFWxhbcLFomvQL\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.20.139, 74.125.20.101, 74.125.20.113, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.20.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0c-cc-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/001vgesanh2321afs9h6lpad5b944kas/1613811750000/13802342090854404605/*/1rY6A0cN_cxAMK3aMHlTFWxhbcLFomvQL [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-02-20 09:02:33--  https://doc-0c-cc-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/001vgesanh2321afs9h6lpad5b944kas/1613811750000/13802342090854404605/*/1rY6A0cN_cxAMK3aMHlTFWxhbcLFomvQL\n",
            "Resolving doc-0c-cc-docs.googleusercontent.com (doc-0c-cc-docs.googleusercontent.com)... 74.125.195.132, 2607:f8b0:400e:c09::84\n",
            "Connecting to doc-0c-cc-docs.googleusercontent.com (doc-0c-cc-docs.googleusercontent.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 219349 (214K) [text/csv]\n",
            "Saving to: â€˜dev.csvâ€™\n",
            "\n",
            "dev.csv             100%[===================>] 214.21K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2021-02-20 09:02:33 (91.1 MB/s) - â€˜dev.csvâ€™ saved [219349/219349]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dn1fa1l3BQc"
      },
      "source": [
        "# Number of epochs\n",
        "epochs = 10\n",
        "\n",
        "# Proportion of training data for train compared to dev\n",
        "train_proportion = 0.8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TCwEEwS3BQd"
      },
      "source": [
        "# We define our training loop\n",
        "def train(train_iter, dev_iter, model, number_epoch):\n",
        "    \"\"\"\n",
        "    Training loop for the model, which calls on eval to evaluate after each epoch\n",
        "    \"\"\"\n",
        "\n",
        "    \n",
        "    print(\"Training model.\")\n",
        "\n",
        "    for epoch in range(1, number_epoch+1):\n",
        "\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        epoch_sse = 0\n",
        "        no_observations = 0  # Observations used for training so far\n",
        "\n",
        "        for batch in train_iter:\n",
        "\n",
        "            feature, target = batch\n",
        "\n",
        "            feature, target = feature.to(device), target.to(device)\n",
        "\n",
        "            # for RNN:\n",
        "            model.batch_size = target.shape[0]\n",
        "            no_observations = no_observations + target.shape[0]\n",
        "            model.hidden = model.init_hidden()\n",
        "\n",
        "            predictions = model(feature).squeeze(1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss = loss_fn(predictions, target)\n",
        "\n",
        "            sse, __ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()*target.shape[0]\n",
        "            epoch_sse += sse\n",
        "\n",
        "        valid_loss, valid_mse, __, __ = eval(dev_iter, model)\n",
        "\n",
        "        epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\n",
        "        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} | Train MSE: {epoch_mse:.2f} | Train RMSE: {epoch_mse**0.5:.2f} | \\\n",
        "        Val. Loss: {valid_loss:.2f} | Val. MSE: {valid_mse:.2f} |  Val. RMSE: {valid_mse**0.5:.2f} |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJeBxYW93BQd"
      },
      "source": [
        "# We evaluate performance on our dev set\n",
        "def eval(data_iter, model):\n",
        "    \"\"\"\n",
        "    Evaluating model performance on the dev set\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_sse = 0\n",
        "    pred_all = []\n",
        "    trg_all = []\n",
        "    no_observations = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_iter:\n",
        "            feature, target = batch\n",
        "\n",
        "            feature, target = feature.to(device), target.to(device)\n",
        "\n",
        "            # for RNN:\n",
        "            model.batch_size = target.shape[0]\n",
        "            no_observations = no_observations + target.shape[0]\n",
        "            model.hidden = model.init_hidden()\n",
        "\n",
        "            predictions = model(feature).squeeze(1)\n",
        "            loss = loss_fn(predictions, target)\n",
        "\n",
        "            # We get the mse\n",
        "            pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n",
        "            sse, __ = model_performance(pred, trg)\n",
        "\n",
        "            epoch_loss += loss.item()*target.shape[0]\n",
        "            epoch_sse += sse\n",
        "            pred_all.extend(pred)\n",
        "            trg_all.extend(trg)\n",
        "\n",
        "    return epoch_loss/no_observations, epoch_sse/no_observations, np.array(pred_all), np.array(trg_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO4PMTUl3BQd"
      },
      "source": [
        "# How we print the model performance\n",
        "def model_performance(output, target, print_output=False):\n",
        "    \"\"\"\n",
        "    Returns SSE and MSE per batch (printing the MSE and the RMSE)\n",
        "    \"\"\"\n",
        "\n",
        "    sq_error = (output - target)**2\n",
        "\n",
        "    sse = np.sum(sq_error)\n",
        "    mse = np.mean(sq_error)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    if print_output:\n",
        "        print(f'| MSE: {mse:.2f} | RMSE: {rmse:.2f} |')\n",
        "\n",
        "    return sse, mse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkYFRpHt3BQe"
      },
      "source": [
        "def create_vocab(data):\n",
        "    \"\"\"\n",
        "    Creating a corpus of all the tokens used\n",
        "    \"\"\"\n",
        "    tokenized_corpus = [] # Let us put the tokenized corpus in a list\n",
        "\n",
        "    for sentence in data:\n",
        "\n",
        "        tokenized_sentence = []\n",
        "\n",
        "        for token in sentence.split(' '): # simplest split is\n",
        "\n",
        "            tokenized_sentence.append(token)\n",
        "\n",
        "        tokenized_corpus.append(tokenized_sentence)\n",
        "\n",
        "    # Create single list of all vocabulary\n",
        "    vocabulary = []  # Let us put all the tokens (mostly words) appearing in the vocabulary in a list\n",
        "\n",
        "    for sentence in tokenized_corpus:\n",
        "\n",
        "        for token in sentence:\n",
        "\n",
        "            if token not in vocabulary:\n",
        "\n",
        "                if True:\n",
        "                    vocabulary.append(token)\n",
        "\n",
        "    return vocabulary, tokenized_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_xUhSV43BQe"
      },
      "source": [
        "def softmax_mask(batch, mask):\n",
        "    normalizing_mask = torch.Tensor([[float('-inf') if token == 0 else 0 for token in entry] for entry in mask]).to(device)\n",
        "    return torch.nn.functional.softmax(batch + normalizing_mask, dim=-1)\n",
        "\n",
        "def padd_mask(batch):\n",
        "    return torch.Tensor([[0 if token == 0 else 1 for token in entry] for entry in batch]).to(device)\n",
        "\n",
        "def collate_fn_padd(batch):\n",
        "    '''\n",
        "    We add padding to our minibatches and create tensors for our model\n",
        "    '''\n",
        "\n",
        "    batch_labels = [l for f, l in batch]\n",
        "    batch_features = [f for f, l in batch]\n",
        "\n",
        "    batch_features_len = [len(f) for f, l in batch]\n",
        "\n",
        "    seq_tensor = torch.zeros((len(batch), max(batch_features_len))).long()\n",
        "\n",
        "    for idx, (seq, seqlen) in enumerate(zip(batch_features, batch_features_len)):\n",
        "        seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
        "\n",
        "    batch_labels = torch.FloatTensor(batch_labels)\n",
        "\n",
        "    return seq_tensor, batch_labels\n",
        "\n",
        "class Task1Dataset(Dataset):\n",
        "\n",
        "    def __init__(self, train_data, labels):\n",
        "        self.x_train = train_data\n",
        "        self.y_train = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y_train)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.x_train[item], self.y_train[item]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GZImPskBk-n"
      },
      "source": [
        "\r\n",
        "\r\n",
        "def mask_softmax(matrix, mask=None):\r\n",
        "    \"\"\"Perform softmax on length dimension with masking.\r\n",
        "\r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    matrix: torch.float, shape [batch_size, .., max_len]\r\n",
        "    mask: torch.long, shape [batch_size, max_len]\r\n",
        "        Mask tensor for sequence.\r\n",
        "\r\n",
        "    Returns\r\n",
        "    -------\r\n",
        "    output: torch.float, shape [batch_size, .., max_len]\r\n",
        "        Normalized output in length dimension.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    if mask is None:\r\n",
        "        result = F.softmax(matrix, dim=-1)\r\n",
        "    else:\r\n",
        "        mask_norm = ((1 - mask) * NEG_INF).to(matrix)\r\n",
        "        for i in range(matrix.dim() - mask_norm.dim()):\r\n",
        "            mask_norm = mask_norm.unsqueeze(1)\r\n",
        "        result = F.softmax(matrix + mask_norm, dim=-1)\r\n",
        "\r\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9nQpept3BQg"
      },
      "source": [
        "class BiLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, batch_size, device):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.device = device\n",
        "        self.batch_size = batch_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2label = nn.Linear(hidden_dim * 2, 1)\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        # Before we've done anything, we dont have any hidden state.\n",
        "        # Refer to the Pytorch documentation to see exactly why they have this dimensionality.\n",
        "        # The axes semantics are (num_layers * num_directions, minibatch_size, hidden_dim)\n",
        "        return torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device), \\\n",
        "               torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embedded = self.embedding(sentence)\n",
        "        embedded = embedded.permute(1, 0, 2)\n",
        "\n",
        "        lstm_out, self.hidden = self.lstm(\n",
        "            embedded.view(len(embedded), self.batch_size, self.embedding_dim), self.hidden)\n",
        "\n",
        "        out = self.hidden2label(lstm_out[-1])\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "0_4Benjk3BQg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "f8373437-4660-466e-a41c-cbf8815287cc"
      },
      "source": [
        "##Â Approach 1 code, using functions defined above:\n",
        "\n",
        "# We set our training data and test data\n",
        "training_data = train_df['original']\n",
        "test_data = test_df['original']\n",
        "\n",
        "# Creating word vectors\n",
        "training_vocab, training_tokenized_corpus = create_vocab(training_data)\n",
        "test_vocab, test_tokenized_corpus = create_vocab(test_data)\n",
        "\n",
        "# Creating joint vocab from test and train:\n",
        "joint_vocab, joint_tokenized_corpus = create_vocab(pd.concat([training_data, test_data]))\n",
        "\n",
        "print(\"Vocab created.\")\n",
        "\n",
        "# We create representations for our tokens\n",
        "wvecs = [] # word vectors\n",
        "word2idx = [] # word2index\n",
        "idx2word = []\n",
        "\n",
        "# This is a large file, it will take a while to load in the memory!\n",
        "with codecs.open('glove.6B.100d.txt', 'r','utf-8') as f:\n",
        "  index = 1\n",
        "  for line in f.readlines():\n",
        "    # Ignore the first line - first line typically contains vocab, dimensionality\n",
        "    if len(line.strip().split()) > 3:\n",
        "      word = line.strip().split()[0]\n",
        "      if word in joint_vocab:\n",
        "          (word, vec) = (word,\n",
        "                     list(map(float,line.strip().split()[1:])))\n",
        "          wvecs.append(vec)\n",
        "          word2idx.append((word, index))\n",
        "          idx2word.append((index, word))\n",
        "          index += 1\n",
        "\n",
        "wvecs = np.array(wvecs)\n",
        "word2idx = dict(word2idx)\n",
        "idx2word = dict(idx2word)\n",
        "\n",
        "vectorized_seqs = [[word2idx[tok] for tok in seq if tok in word2idx] for seq in training_tokenized_corpus]\n",
        "\n",
        "# To avoid any sentences being empty (if no words match to our word embeddings)\n",
        "vectorized_seqs = [x if len(x) > 0 else [0] for x in vectorized_seqs]\n",
        "\n",
        "INPUT_DIM = len(word2idx)\n",
        "EMBEDDING_DIM = 100\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "model = BiLSTM(EMBEDDING_DIM, 50, INPUT_DIM, BATCH_SIZE, device)\n",
        "print(\"Model initialised.\")\n",
        "\n",
        "model.to(device)\n",
        "# We provide the model with our embeddings\n",
        "model.embedding.weight.data.copy_(torch.from_numpy(wvecs))\n",
        "\n",
        "feature = vectorized_seqs\n",
        "\n",
        "# 'feature' is a list of lists, each containing embedding IDs for word tokens\n",
        "train_and_dev = Task1Dataset(feature, train_df['meanGrade'])\n",
        "\n",
        "train_examples = round(len(train_and_dev)*train_proportion)\n",
        "dev_examples = len(train_and_dev) - train_examples\n",
        "\n",
        "train_dataset, dev_dataset = random_split(train_and_dev,\n",
        "                                           (train_examples,\n",
        "                                            dev_examples))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\n",
        "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\n",
        "\n",
        "print(\"Dataloaders created.\")\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "loss_fn = loss_fn.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "train(train_loader, dev_loader, model, epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab created.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-31f16527d1f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# This is a large file, it will take a while to load in the memory!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove.6B.100d.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, encoding, errors, buffering)\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[0;31m# Force opening of the file in binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B.100d.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcmPkDNBxYQy"
      },
      "source": [
        "class BERT(nn.Module):\r\n",
        "    def __init__(self, embedding_dim, hidden_dim, batch_size, device):\r\n",
        "        super(BERT, self).__init__()\r\n",
        "        self.hidden_dim = hidden_dim\r\n",
        "        self.embedding_dim = embedding_dim\r\n",
        "        self.device = device\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\r\n",
        "\r\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\r\n",
        "        # with dimensionality hidden_dim.\r\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\r\n",
        "\r\n",
        "        # The linear layer that maps from hidden state space to tag space\r\n",
        "        self.hidden2label = nn.Linear(hidden_dim * 2, 1)\r\n",
        "        self.hidden = self.init_hidden()\r\n",
        "\r\n",
        "        self.attn_linear = nn.Linear(hidden_dim * 2, 1)\r\n",
        "\r\n",
        "    def init_hidden(self):\r\n",
        "        # Before we've done anything, we dont have any hidden state.\r\n",
        "        # Refer to the Pytorch documentation to see exactly why they have this dimensionality.\r\n",
        "        # The axes semantics are (num_layers * num_directions, minibatch_size, hidden_dim)\r\n",
        "        return torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device), \\\r\n",
        "               torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device)\r\n",
        "\r\n",
        "    def get_embedded(self, sentence):\r\n",
        "\r\n",
        "        # # Run the text through BERT, and collect all of the hidden states produced\r\n",
        "        # # from all 12 layers. \r\n",
        "        # with torch.no_grad():\r\n",
        "\r\n",
        "        #     outputs = model(tokens_tensor, segments_tensors)\r\n",
        "\r\n",
        "        #     # Evaluating the model will return a different number of objects based on \r\n",
        "        #     # how it's  configured in the `from_pretrained` call earlier. In this case, \r\n",
        "        #     # becase we set `output_hidden_states = True`, the third item will be the \r\n",
        "        #     # hidden states from all layers. See the documentation for more details:\r\n",
        "        #     # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\r\n",
        "        #     hidden_states = outputs[2]\r\n",
        "\r\n",
        "        # # `hidden_states` has shape [13 x 1 x 22 x 768]\r\n",
        "\r\n",
        "        # # `token_vecs` is a tensor with shape [22 x 768]\r\n",
        "        # token_vecs = hidden_states[-2][0]\r\n",
        "\r\n",
        "        # # Calculate the average of all 22 token vectors.\r\n",
        "        # sentence_embedding = torch.mean(token_vecs, dim=0)\r\n",
        "\r\n",
        "        return last_hidden_states\r\n",
        "\r\n",
        "    def forward(self, sentence):\r\n",
        "        mask = padd_mask(sentence)\r\n",
        "\r\n",
        "        # Get word embeddings. BERT base gives us 768 hidden parameters for each word.\r\n",
        "        # (batch_size, max_len) -> (batch_size, max_len, 768)\r\n",
        "        embedded = self.bert(sentence, attention_mask=mask)[0] # The last hidden-state is the first element of the output tuple        \r\n",
        "        # (batch_size, max_len) -> (max_len, batch_size, 768)\r\n",
        "        embedded = embedded.permute(1, 0, 2)\r\n",
        "\r\n",
        "        # Do I still need this?\r\n",
        "        # (max_len, batch_size, 768) -> (max_len, batch_size, directions * hidden_dim)\r\n",
        "        lstm_out, self.hidden = self.lstm(\r\n",
        "            embedded.view(len(embedded), self.batch_size, self.embedding_dim), self.hidden)\r\n",
        "        \r\n",
        "        # Attention Mechanism\r\n",
        "\r\n",
        "        # (max_len, batch_size, directions * hidden_dim) -> (batch_size, max_len, directions * hidden_dim)\r\n",
        "        lstm_out = lstm_out.permute(1, 0, 2)\r\n",
        "        # Get similarity using DOT attention (i think)\r\n",
        "        # (batch_size, max_len, directions * hidden_dim) -> (batch_size, max_len)\r\n",
        "        att_out = self.attn_linear(lstm_out).squeeze(-1)\r\n",
        "        # Get the attention weights for each token in a sentence (batch_size)\r\n",
        "        # (batch_size, max_len) -> (batch_size, max_len)\r\n",
        "        # att_out = torch.nn.functional.softmax(att_out, dim=-1)\r\n",
        "        att_out = softmax_mask(att_out, mask)\r\n",
        "        # Get sentence vector which is a weighted sum of token hidden states\r\n",
        "        # (batch_size, max_len) -> (batch_size, directions * hidden_dim)\r\n",
        "        att_out = torch.sum(att_out.unsqueeze(-1) * lstm_out, dim=1)\r\n",
        "        \r\n",
        "        # out = self.hidden2label(lstm_out[-1])\r\n",
        "        out = self.hidden2label(att_out)\r\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK_8btev5GEw"
      },
      "source": [
        "import re\r\n",
        "\r\n",
        "# Extract the original and new sentences + words from a dataframe\r\n",
        "def get_sentences(df, og_label='original', edit_label='edit'):\r\n",
        "  p = r\"<(.*)\\/>\"\r\n",
        "  replace_regex = re.compile(p, re.UNICODE)\r\n",
        "  og_word = []\r\n",
        "  new_word = []\r\n",
        "  og_sentences = []\r\n",
        "  new_sentences = []\r\n",
        "\r\n",
        "  for s, w in df[[og_label, edit_label]].itertuples(index=False,name=None):\r\n",
        "    tokens = s.split(' ') # For each sentence get the words\r\n",
        "    m = replace_regex.search(str(s)) # Get the word to replace\r\n",
        "\r\n",
        "    assert not m is None # Couldn't regex match the replacement word\r\n",
        "\r\n",
        "    og_word.append(m.group(1))\r\n",
        "    new_word.append(w)\r\n",
        "    og_sentences.append(replace_regex.sub( m.group(1), s))\r\n",
        "    new_sentences.append(replace_regex.sub(w, s))\r\n",
        "  \r\n",
        "  return og_sentences, new_sentences, og_word, new_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBr1rcmd_fvd",
        "outputId": "7fb06b4c-8dc4-4ca1-e663-ff445ca3e10c"
      },
      "source": [
        "# Normal Training\r\n",
        "\r\n",
        "##Â Approach 1 code, using functions defined above:\r\n",
        "\r\n",
        "# We set our training data and test data\r\n",
        "# training_data = train_df['original']\r\n",
        "test_data = test_df['original']\r\n",
        "\r\n",
        "_, training_data, _, _ = get_sentences(train_df)\r\n",
        "\r\n",
        "EMBEDDING_DIM = 768\r\n",
        "BATCH_SIZE = 32\r\n",
        "\r\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\r\n",
        "vectorized_seqs = [tokenizer.encode(tokenizer.tokenize(sentence)) for sentence in training_data]\r\n",
        "# input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\r\n",
        "\r\n",
        "model = BERT(EMBEDDING_DIM, 50, BATCH_SIZE, device)\r\n",
        "model.to(device)\r\n",
        "print(\"Model initialised.\")\r\n",
        "\r\n",
        "train_and_dev = Task1Dataset(vectorized_seqs, train_df['meanGrade'])\r\n",
        "\r\n",
        "train_examples = round(len(train_and_dev)*train_proportion)\r\n",
        "dev_examples = len(train_and_dev) - train_examples\r\n",
        "\r\n",
        "train_dataset, dev_dataset = random_split(train_and_dev,\r\n",
        "                                           (train_examples,\r\n",
        "                                            dev_examples))\r\n",
        "\r\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\r\n",
        "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\r\n",
        "\r\n",
        "print(\"Dataloaders created.\")\r\n",
        "\r\n",
        "loss_fn = nn.MSELoss()\r\n",
        "loss_fn = loss_fn.to(device)\r\n",
        "\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), 0.00001)\r\n",
        "\r\n",
        "train(train_loader, dev_loader, model, epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model initialised.\n",
            "Dataloaders created.\n",
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.56 |\n",
            "| Epoch: 02 | Train Loss: 0.27 | Train MSE: 0.27 | Train RMSE: 0.52 |         Val. Loss: 0.30 | Val. MSE: 0.30 |  Val. RMSE: 0.54 |\n",
            "| Epoch: 03 | Train Loss: 0.21 | Train MSE: 0.21 | Train RMSE: 0.46 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.55 |\n",
            "| Epoch: 04 | Train Loss: 0.16 | Train MSE: 0.16 | Train RMSE: 0.40 |         Val. Loss: 0.33 | Val. MSE: 0.33 |  Val. RMSE: 0.57 |\n",
            "| Epoch: 05 | Train Loss: 0.12 | Train MSE: 0.12 | Train RMSE: 0.35 |         Val. Loss: 0.35 | Val. MSE: 0.35 |  Val. RMSE: 0.59 |\n",
            "| Epoch: 06 | Train Loss: 0.09 | Train MSE: 0.09 | Train RMSE: 0.31 |         Val. Loss: 0.36 | Val. MSE: 0.36 |  Val. RMSE: 0.60 |\n",
            "| Epoch: 07 | Train Loss: 0.08 | Train MSE: 0.08 | Train RMSE: 0.27 |         Val. Loss: 0.36 | Val. MSE: 0.36 |  Val. RMSE: 0.60 |\n",
            "| Epoch: 08 | Train Loss: 0.06 | Train MSE: 0.06 | Train RMSE: 0.25 |         Val. Loss: 0.37 | Val. MSE: 0.37 |  Val. RMSE: 0.61 |\n",
            "| Epoch: 09 | Train Loss: 0.05 | Train MSE: 0.05 | Train RMSE: 0.23 |         Val. Loss: 0.38 | Val. MSE: 0.38 |  Val. RMSE: 0.61 |\n",
            "| Epoch: 10 | Train Loss: 0.05 | Train MSE: 0.05 | Train RMSE: 0.22 |         Val. Loss: 0.39 | Val. MSE: 0.39 |  Val. RMSE: 0.62 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkkhjM0tirRb"
      },
      "source": [
        "# Cross Validation\r\n",
        "\r\n",
        "from sklearn.model_selection import ParameterGrid, KFold\r\n",
        "\r\n",
        "##Â Approach 1 code, using functions defined above:\r\n",
        "\r\n",
        "_, training_data, _, _ = get_sentences(train_df)\r\n",
        "\r\n",
        "EMBEDDING_DIM = 768\r\n",
        "BATCH_SIZE = 32\r\n",
        "\r\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\r\n",
        "vectorized_seqs = [tokenizer.encode(tokenizer.tokenize(sentence)) for sentence in training_data]\r\n",
        "\r\n",
        "model = BERT(EMBEDDING_DIM, 50, BATCH_SIZE, device)\r\n",
        "model.to(device)\r\n",
        "print(\"Model initialised.\")\r\n",
        "\r\n",
        "loss_fn = nn.MSELoss()\r\n",
        "loss_fn = loss_fn.to(device)\r\n",
        "\r\n",
        "grid = {\r\n",
        "    'learning_rate': [0.0001, 0.00001]\r\n",
        "}\r\n",
        "\r\n",
        "# scorer = make_scorer(make_custom_score, needs_proba=True)\r\n",
        "paramGrid = ParameterGrid(grid)\r\n",
        "cv = KFold(5)\r\n",
        "labels = train_df['meanGrade']\r\n",
        "\r\n",
        "train_and_dev = Task1Dataset(vectorized_seqs, train_df['meanGrade'])\r\n",
        "\r\n",
        "# Cross Validation\r\n",
        "\r\n",
        "for params in paramGrid:\r\n",
        "\r\n",
        "    print(\"Current parameters: \" + str(params))\r\n",
        "    fold = 0;\r\n",
        "\r\n",
        "    optimizer = torch.optim.Adam(model.parameters(), params[\"learning_rate\"])\r\n",
        "    for train_indices, dev_indices in cv.split(vectorized_seqs, labels):\r\n",
        "\r\n",
        "        fold += 1;\r\n",
        "        print(\"Current fold: \" + str(fold))\r\n",
        "\r\n",
        "        train_dataset = torch.utils.data.Subset(train_and_dev, train_indices)\r\n",
        "        dev_dataset = torch.utils.data.Subset(train_and_dev, dev_indices)\r\n",
        "\r\n",
        "        print(\"Dataloaders created.\")\r\n",
        "\r\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\r\n",
        "        dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\r\n",
        "\r\n",
        "        train(train_loader, dev_loader, model, epochs)\r\n",
        "\r\n",
        "        # Calculate some kind of model score here?\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lJqtIbrhp7C"
      },
      "source": [
        "class RoBERTa(nn.Module):\r\n",
        "    def __init__(self, embedding_dim, hidden_dim, batch_size, device):\r\n",
        "        super(RoBERTa, self).__init__()\r\n",
        "        self.hidden_dim = hidden_dim\r\n",
        "        self.embedding_dim = embedding_dim\r\n",
        "        self.device = device\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.roberta = model = RobertaModel.from_pretrained('roberta-base')\r\n",
        "\r\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\r\n",
        "        # with dimensionality hidden_dim.\r\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\r\n",
        "\r\n",
        "        # The linear layer that maps from hidden state space to tag space\r\n",
        "        self.hidden2label = nn.Linear(hidden_dim * 2, 1)\r\n",
        "        self.hidden = self.init_hidden()\r\n",
        "\r\n",
        "    def init_hidden(self):\r\n",
        "        # Before we've done anything, we dont have any hidden state.\r\n",
        "        # Refer to the Pytorch documentation to see exactly why they have this dimensionality.\r\n",
        "        # The axes semantics are (num_layers * num_directions, minibatch_size, hidden_dim)\r\n",
        "        return torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device), \\\r\n",
        "               torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device)\r\n",
        "\r\n",
        "    def get_embedded(self, sentence):\r\n",
        "\r\n",
        "        outputs = self.roberta(sentence)\r\n",
        "        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\r\n",
        "\r\n",
        "        # # Run the text through BERT, and collect all of the hidden states produced\r\n",
        "        # # from all 12 layers. \r\n",
        "        # with torch.no_grad():\r\n",
        "\r\n",
        "        #     outputs = model(tokens_tensor, segments_tensors)\r\n",
        "\r\n",
        "        #     # Evaluating the model will return a different number of objects based on \r\n",
        "        #     # how it's  configured in the `from_pretrained` call earlier. In this case, \r\n",
        "        #     # becase we set `output_hidden_states = True`, the third item will be the \r\n",
        "        #     # hidden states from all layers. See the documentation for more details:\r\n",
        "        #     # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\r\n",
        "        #     hidden_states = outputs[2]\r\n",
        "\r\n",
        "        # # `hidden_states` has shape [13 x 1 x 22 x 768]\r\n",
        "\r\n",
        "        # # `token_vecs` is a tensor with shape [22 x 768]\r\n",
        "        # token_vecs = hidden_states[-2][0]\r\n",
        "\r\n",
        "        # # Calculate the average of all 22 token vectors.\r\n",
        "        # sentence_embedding = torch.mean(token_vecs, dim=0)\r\n",
        "\r\n",
        "        return last_hidden_states\r\n",
        "\r\n",
        "    def forward(self, sentence):\r\n",
        "\r\n",
        "        embedded = self.get_embedded(sentence)\r\n",
        "        embedded = embedded.permute(1, 0, 2)\r\n",
        "\r\n",
        "        # Do I still need this if I'm also training the BERT model?\r\n",
        "        lstm_out, self.hidden = self.lstm(\r\n",
        "            embedded.view(len(embedded), self.batch_size, self.embedding_dim), self.hidden)\r\n",
        "\r\n",
        "        out = self.hidden2label(lstm_out[-1])\r\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434,
          "referenced_widgets": [
            "8762ac9342f44c26b866e7a8ad174594",
            "bb9dead128774fdfb6e910ff3dfa8324",
            "b69e8a70ff1c41fbaf623892e0efc39b",
            "ce41a07683fb4e5a93d21b5208ee2e2b",
            "6b745844d2fc4458a17500f37e20876f",
            "f17e3e7121d443ae88bbbec6f2f417d7",
            "0ef8dcd1f2194af8b8989b4d1dd27fe3",
            "ba90843ee83a4711817a25af21b73e11",
            "c3f8e38c826749d0ab821e519773d3e5",
            "1e42b02f8951449eb3dd0cb7e193be57",
            "6be2fd8ccb114e2f9e701d0053f50a9a",
            "1a7f3085d61144e6828f40f9bde4b321",
            "0b55f05ca46b49c4832efa86512afb5d",
            "43768c2dc1dd4609896bd34d1757a005",
            "7f4616f9a80e4a26a1b1266c84c5776d",
            "f1b001d914d845b0b6d47137b5c42a38",
            "741fd16424c54f8b89324c8b2058daf7",
            "b5793bcfe41d48198fbabeaea96b38a8",
            "f450c9f588b24c788630c7678d664fef",
            "5892e97cab574a729e9597b7a2b67b47",
            "af175e27d00d4a648327ee404a50249a",
            "c939ae3ce69c44abae1b659222fa382f",
            "f7513219a2cc4100a30958750bbac674",
            "bd8b56328ad747d0afa8e62139778373",
            "fd7e20a0c1a14ff0980544b3e28da5c9",
            "c4bcc7fd11c84289b4e35739cfd239cf",
            "e73980c9a63b4bacbea15dad1d7f471b",
            "12b34fec2fc34a65ab7a7b5f2176b7fc",
            "de49df14f18b446eb84e6b3a428b5d15",
            "4f1f45255acf490988852ab8aaf42290",
            "3ba08cd0cf964391a1a6485de6946f8a",
            "3f2fe1c785d9448ca603771dd8a63e4a"
          ]
        },
        "id": "zV9WJUU_hqLh",
        "outputId": "caa02473-6ebe-4e27-c568-96956db7652f"
      },
      "source": [
        "##Â Approach 1 code, using functions defined above:\r\n",
        "\r\n",
        "# We set our training data and test data\r\n",
        "training_data = train_df['original']\r\n",
        "test_data = test_df['original']\r\n",
        "\r\n",
        "vectorized_seqs = [[word2idx[tok] for tok in seq if tok in word2idx] for seq in training_tokenized_corpus]\r\n",
        "\r\n",
        "# To avoid any sentences being empty (if no words match to our word embeddings)\r\n",
        "vectorized_seqs = [x if len(x) > 0 else [0] for x in vectorized_seqs]\r\n",
        "\r\n",
        "EMBEDDING_DIM = 768\r\n",
        "BATCH_SIZE = 32\r\n",
        "\r\n",
        "tokenizer = tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\r\n",
        "vectorized_seqs = [tokenizer.encode(sentence) for sentence in training_data]\r\n",
        "# input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\r\n",
        "\r\n",
        "model = RoBERTa(EMBEDDING_DIM, 50, BATCH_SIZE, device)\r\n",
        "model.to(device)\r\n",
        "print(\"Model initialised.\")\r\n",
        "\r\n",
        "train_and_dev = Task1Dataset(vectorized_seqs, train_df['meanGrade'])\r\n",
        "\r\n",
        "train_examples = round(len(train_and_dev)*train_proportion)\r\n",
        "dev_examples = len(train_and_dev) - train_examples\r\n",
        "\r\n",
        "train_dataset, dev_dataset = random_split(train_and_dev,\r\n",
        "                                           (train_examples,\r\n",
        "                                            dev_examples))\r\n",
        "\r\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\r\n",
        "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\r\n",
        "\r\n",
        "print(\"Dataloaders created.\")\r\n",
        "\r\n",
        "loss_fn = nn.MSELoss()\r\n",
        "loss_fn = loss_fn.to(device)\r\n",
        "\r\n",
        "optimizer = torch.optim.Adam(model.parameters())\r\n",
        "\r\n",
        "train(train_loader, dev_loader, model, epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8762ac9342f44c26b866e7a8ad174594",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3f8e38c826749d0ab821e519773d3e5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "741fd16424c54f8b89324c8b2058daf7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=481.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd7e20a0c1a14ff0980544b3e28da5c9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=501200538.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Model initialised.\n",
            "Dataloaders created.\n",
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.58 |\n",
            "| Epoch: 02 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.59 |\n",
            "| Epoch: 03 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.59 |\n",
            "| Epoch: 04 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.58 |\n",
            "| Epoch: 05 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.58 |\n",
            "| Epoch: 06 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.59 |\n",
            "| Epoch: 07 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.58 |\n",
            "| Epoch: 08 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.58 |\n",
            "| Epoch: 09 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.59 |\n",
            "| Epoch: 10 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.58 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahuFb8KU3BQi"
      },
      "source": [
        "#### Approach 2: No pre-trained representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewkWUrP33BQj"
      },
      "source": [
        "train_and_dev = train_df['edit']\n",
        "\n",
        "training_data, dev_data, training_y, dev_y = train_test_split(train_df['edit'], train_df['meanGrade'],\n",
        "                                                                        test_size=(1-train_proportion),\n",
        "                                                                        random_state=42)\n",
        "\n",
        "# We train a Tf-idf model\n",
        "count_vect = CountVectorizer(stop_words='english')\n",
        "train_counts = count_vect.fit_transform(training_data)\n",
        "transformer = TfidfTransformer().fit(train_counts)\n",
        "train_counts = transformer.transform(train_counts)\n",
        "regression_model = LinearRegression().fit(train_counts, training_y)\n",
        "\n",
        "# Train predictions\n",
        "predicted_train = regression_model.predict(train_counts)\n",
        "\n",
        "# Calculate Tf-idf using train and dev, and validate model on dev:\n",
        "test_and_test_counts = count_vect.transform(train_and_dev)\n",
        "transformer = TfidfTransformer().fit(test_and_test_counts)\n",
        "\n",
        "test_counts = count_vect.transform(dev_data)\n",
        "\n",
        "test_counts = transformer.transform(test_counts)\n",
        "\n",
        "# Dev predictions\n",
        "predicted = regression_model.predict(test_counts)\n",
        "\n",
        "# We run the evaluation:\n",
        "print(\"\\nTrain performance:\")\n",
        "sse, mse = model_performance(predicted_train, training_y, True)\n",
        "\n",
        "print(\"\\nDev performance:\")\n",
        "sse, mse = model_performance(predicted, dev_y, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA5bYyuG3BQj"
      },
      "source": [
        "#### Baseline for task 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72fIC0-I3BQk",
        "outputId": "d01b6da1-0c83-4f30-cdcc-a18c4c21798c"
      },
      "source": [
        "#Â Baseline for the task\n",
        "pred_baseline = torch.zeros(len(dev_y)) + np.mean(training_y)\n",
        "print(\"\\nBaseline performance:\")\n",
        "sse, mse = model_performance(pred_baseline, dev_y, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Baseline performance:\n",
            "| MSE: 0.34 | RMSE: 0.58 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoVfvcAs3BQk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}