{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3UWJ9Z96uay3",
        "r87Ra9hmO0MT",
        "J2f5JkcXRU0h",
        "vLBuf54LaSlZ",
        "xsFTU7vrT7B1",
        "xMA3mumakDSZ"
      ]
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "aadfb751523748d7a69b19fa5064d1e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_918221bb80d24a4b8a676baa22ad83a6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fe7b42baede0437d8559d53d9544d354",
              "IPY_MODEL_fbb5745245de4178854a0e0ed227603b"
            ]
          }
        },
        "918221bb80d24a4b8a676baa22ad83a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fe7b42baede0437d8559d53d9544d354": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e2d445db538e4013b8e699f608d23315",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 481,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 481,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a6e7469b0dba4f49bd5becbd530c3fa8"
          }
        },
        "fbb5745245de4178854a0e0ed227603b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5553c6513b7e44a0938673de57f10e42",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 481/481 [00:09&lt;00:00, 52.1B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_43057c320f31478c884c4ef3beb88fd2"
          }
        },
        "e2d445db538e4013b8e699f608d23315": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a6e7469b0dba4f49bd5becbd530c3fa8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5553c6513b7e44a0938673de57f10e42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "43057c320f31478c884c4ef3beb88fd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e59e87581aaa43cea3fae89cfbd0bdd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6dff85f849e64a7c9a16ad5af717edf3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_18d17e97132c426ea3a2f112fde7695f",
              "IPY_MODEL_5af7239138ec4701a2a4bdd479af07f6"
            ]
          }
        },
        "6dff85f849e64a7c9a16ad5af717edf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "18d17e97132c426ea3a2f112fde7695f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_65bb17d000e14cfc9fc2e5afb0ea79b0",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 501200538,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 501200538,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a1c2272fcdfc48e2959a321ca2cfccc3"
          }
        },
        "5af7239138ec4701a2a4bdd479af07f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fbec9a81efa44b6ebf33147737140f0b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 501M/501M [00:08&lt;00:00, 58.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_67b53d6aff6a4ca5bf196fe1491a7220"
          }
        },
        "65bb17d000e14cfc9fc2e5afb0ea79b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a1c2272fcdfc48e2959a321ca2cfccc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fbec9a81efa44b6ebf33147737140f0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "67b53d6aff6a4ca5bf196fe1491a7220": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWRt9UuE3BQP"
      },
      "source": [
        "### Coursework coding instructions (please also see full coursework spec)\n",
        "\n",
        "Please choose if you want to do either Task 1 or Task 2. You should write your report about one task only.\n",
        "\n",
        "For the task you choose you will need to do two approaches:\n",
        "  - Approach 1, which can use use pre-trained embeddings / models\n",
        "  - Approach 2, which should not use any pre-trained embeddings or models\n",
        "We should be able to run both approaches from the same colab file\n",
        "\n",
        "#### Running your code:\n",
        "  - Your models should run automatically when running your colab file without further intervention\n",
        "  - For each task you should automatically output the performance of both models\n",
        "  - Your code should automatically download any libraries required\n",
        "\n",
        "#### Structure of your code:\n",
        "  - You are expected to use the 'train', 'eval' and 'model_performance' functions, although you may edit these as required\n",
        "  - Otherwise there are no restrictions on what you can do in your code\n",
        "\n",
        "#### Documentation:\n",
        "  - You are expected to produce a .README file summarising how you have approached both tasks\n",
        "\n",
        "#### Reproducibility:\n",
        "  - Your .README file should explain how to replicate the different experiments mentioned in your report\n",
        "\n",
        "Good luck! We are really looking forward to seeing your reports and your model code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCZYhNJ2Czn-"
      },
      "source": [
        "# README\r\n",
        "\r\n",
        "Each cell is labelled/grouped using the text cells above them. This README hopefully allows you to reproduce our results by running the stated cells in that specific order.\r\n",
        "\r\n",
        "## GLoVe\r\n",
        "\r\n",
        "- Start\r\n",
        "- Load Data\r\n",
        "- Training and Evaluation Helpers\r\n",
        "- Helper Functions\r\n",
        "- Models\r\n",
        "- Initialisation & Processing - Prepare the data for training, assign hyperparameters, initialise the models.\r\n",
        "- Initialisation & Processing - Extract and parse the edited words and sentences\r\n",
        "- Training - Models (Approach 1) - GLoVe Models - GLoVe-specific Training\r\n",
        "- GLoVe Testing\r\n",
        "\r\n",
        "\r\n",
        "## BERT Single w/ Attention\r\n",
        "\r\n",
        "- Start\r\n",
        "- Load Data\r\n",
        "- Training and Evaluation Helpers\r\n",
        "- Helper Functions\r\n",
        "- Models\r\n",
        "- Initialisation & Processing - Prepare the data for training, assign hyperparameters, initialise the models.\r\n",
        "- Initialisation & Processing - Extract and parse the edited words and sentences\r\n",
        "- Initialisation & Processing - Prepare Bert and RoBerta specfic Preprocessing\r\n",
        "- BERT Testing\r\n",
        "- Training - Models (Approach 1) - Bert Training with Single Sentence\r\n",
        "\r\n",
        "\r\n",
        "## BERT Single w/ Attention, Freeze\r\n",
        "\r\n",
        "- Start\r\n",
        "- Load Data\r\n",
        "- Training and Evaluation Helpers\r\n",
        "- Helper Functions\r\n",
        "- Models\r\n",
        "- Initialisation & Processing - Prepare the data for training, assign hyperparameters, initialise the models.\r\n",
        "- Initialisation & Processing - Extract and parse the edited words and sentences\r\n",
        "- Initialisation & Processing - Prepare Bert and RoBerta specfic Preprocessing\r\n",
        "- BERT Testing\r\n",
        "- Training - Models (Approach 1) - Bert Training with Single Sentence and Freezing\r\n",
        "\r\n",
        "\r\n",
        "## BERT Double\r\n",
        "\r\n",
        "- Start\r\n",
        "- Load Data\r\n",
        "- Training and Evaluation Helpers\r\n",
        "- Helper Functions\r\n",
        "- Models\r\n",
        "- Initialisation & Processing - Prepare the data for training, assign hyperparameters, initialise the models.\r\n",
        "- Initialisation & Processing - Extract and parse the edited words and sentences\r\n",
        "- Initialisation & Processing - Prepare Bert and RoBerta specfic Preprocessing\r\n",
        "- BERT Testing\r\n",
        "- Training - Models (Approach 1) - Bert Training with Double Sentences\r\n",
        "\r\n",
        "\r\n",
        "## BertForSequenceClassification\r\n",
        "\r\n",
        "- Start\r\n",
        "- Load Data\r\n",
        "- Training and Evaluation Helpers\r\n",
        "- Helper Functions\r\n",
        "- Models\r\n",
        "- Initialisation & Processing - Prepare the data for training, assign hyperparameters, initialise the models.\r\n",
        "- Initialisation & Processing - Extract and parse the edited words and sentences\r\n",
        "- Initialisation & Processing - Prepare Bert and RoBerta specfic Preprocessing\r\n",
        "- BERT Testing\r\n",
        "- Training - Models (Approach 1) - Bert with Sequence Classification with Both Sentences\r\n",
        "\r\n",
        "\r\n",
        "## RoBERTa\r\n",
        "\r\n",
        "- Start\r\n",
        "- Load Data\r\n",
        "- Training and Evaluation Helpers\r\n",
        "- Helper Functions\r\n",
        "- Models\r\n",
        "- Initialisation & Processing - Prepare the data for training, assign hyperparameters, initialise the models.\r\n",
        "- Initialisation & Processing - Extract and parse the edited words and sentences\r\n",
        "- Initialisation & Processing - Prepare Bert and RoBerta specfic Preprocessing\r\n",
        "- RoBERTa Testing\r\n",
        "- Training - Models (Approach 1) - RoBERTa Single\r\n",
        "\r\n",
        "\r\n",
        "## RoBERTa Double\r\n",
        "\r\n",
        "- Start\r\n",
        "- Load Data\r\n",
        "- Training and Evaluation Helpers\r\n",
        "- Helper Functions\r\n",
        "- Models\r\n",
        "- Initialisation & Processing - Prepare the data for training, assign hyperparameters, initialise the models.\r\n",
        "- Initialisation & Processing - Extract and parse the edited words and sentences\r\n",
        "- Initialisation & Processing - Prepare Bert and RoBerta specfic Preprocessing\r\n",
        "- RoBERTa Testing\r\n",
        "- Training - Models (Approach 1) - RoBERTa Double\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZgNBjWICdZY"
      },
      "source": [
        "# Start\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXHhn8ABipCS",
        "outputId": "0530966e-0ea3-4ea7-8c47-d8dc43ab966a"
      },
      "source": [
        "!pip install transformers\r\n",
        "!pip install torch\r\n",
        "!pip install skorch"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.3.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.7.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: skorch in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from skorch) (0.22.2.post1)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.7/dist-packages (from skorch) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from skorch) (1.19.5)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from skorch) (0.8.9)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from skorch) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->skorch) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL0-upkD3BQY"
      },
      "source": [
        "# Imports\n",
        "import sys\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from transformers import BertTokenizer, RobertaTokenizer, AdamW, BertConfig\n",
        "from transformers import BertModel, BertForSequenceClassification,  RobertaModel\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import codecs\n",
        "import gc"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ov0JGF5J3BQZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd7ee6e2-7cea-48bd-bb30-18363a001857"
      },
      "source": [
        "# Setting random seed and device\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "if use_cuda:\n",
        "  print(\"Using GPU.\")\n",
        "else:\n",
        "  print(\"Using CPU.\")\n",
        "\n",
        "def reset_seed(SEED=1):\n",
        "  torch.manual_seed(SEED)\n",
        "  torch.cuda.manual_seed(SEED)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  np.random.seed(SEED)\n",
        "\n",
        "reset_seed()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvdvG84NCqiG"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oZQFbuF3BQa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1ffc750-ef8b-4d31-abc9-462b90718093"
      },
      "source": [
        "# Load data\n",
        "\n",
        "!wget -O train.csv https://drive.google.com/u/0/uc?id=1UgrdjcHHZmAthjusQDAKoSqd37up-41f&export=download\n",
        "!wget -O dev.csv https://drive.google.com/u/0/uc?id=1rY6A0cN_cxAMK3aMHlTFWxhbcLFomvQL&export=download\n",
        "!wget -O test.csv https://drive.google.com/u/0/uc?id=1JBovQj-Tki8k2yJxLFxexUvs3ahrCneu&export=download\n",
        "\n",
        "train_df = pd.read_csv('./train.csv')\n",
        "test_df = pd.read_csv('./dev.csv')\n",
        "true_test_df = pd.read_csv('./test.csv')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-02 13:49:05--  https://drive.google.com/u/0/uc?id=1UgrdjcHHZmAthjusQDAKoSqd37up-41f\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.20.139, 74.125.20.102, 74.125.20.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.20.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-00-cc-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/vf7u4gu1voud67vh5tji3h23gfevkqp0/1614692925000/13802342090854404605/*/1UgrdjcHHZmAthjusQDAKoSqd37up-41f [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-03-02 13:49:05--  https://doc-00-cc-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/vf7u4gu1voud67vh5tji3h23gfevkqp0/1614692925000/13802342090854404605/*/1UgrdjcHHZmAthjusQDAKoSqd37up-41f\n",
            "Resolving doc-00-cc-docs.googleusercontent.com (doc-00-cc-docs.googleusercontent.com)... 74.125.195.132, 2607:f8b0:400e:c09::84\n",
            "Connecting to doc-00-cc-docs.googleusercontent.com (doc-00-cc-docs.googleusercontent.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 947914 (926K) [text/csv]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "train.csv           100%[===================>] 925.70K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2021-03-02 13:49:06 (156 MB/s) - ‘train.csv’ saved [947914/947914]\n",
            "\n",
            "--2021-03-02 13:49:06--  https://drive.google.com/u/0/uc?id=1rY6A0cN_cxAMK3aMHlTFWxhbcLFomvQL\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.20.101, 74.125.20.102, 74.125.20.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.20.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0c-cc-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/d7safviuusdssol6cjc28v07gvuksglc/1614692925000/13802342090854404605/*/1rY6A0cN_cxAMK3aMHlTFWxhbcLFomvQL [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-03-02 13:49:06--  https://doc-0c-cc-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/d7safviuusdssol6cjc28v07gvuksglc/1614692925000/13802342090854404605/*/1rY6A0cN_cxAMK3aMHlTFWxhbcLFomvQL\n",
            "Resolving doc-0c-cc-docs.googleusercontent.com (doc-0c-cc-docs.googleusercontent.com)... 74.125.195.132, 2607:f8b0:400e:c09::84\n",
            "Connecting to doc-0c-cc-docs.googleusercontent.com (doc-0c-cc-docs.googleusercontent.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 219349 (214K) [text/csv]\n",
            "Saving to: ‘dev.csv’\n",
            "\n",
            "dev.csv             100%[===================>] 214.21K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2021-03-02 13:49:07 (127 MB/s) - ‘dev.csv’ saved [219349/219349]\n",
            "\n",
            "--2021-03-02 13:49:07--  https://drive.google.com/u/0/uc?id=1JBovQj-Tki8k2yJxLFxexUvs3ahrCneu\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.20.101, 74.125.20.102, 74.125.20.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.20.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0c-3g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/kldc0sorr9f87jarkrep7e7jqoffvqmg/1614692925000/04852497093446392078/*/1JBovQj-Tki8k2yJxLFxexUvs3ahrCneu [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-03-02 13:49:08--  https://doc-0c-3g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/kldc0sorr9f87jarkrep7e7jqoffvqmg/1614692925000/04852497093446392078/*/1JBovQj-Tki8k2yJxLFxexUvs3ahrCneu\n",
            "Resolving doc-0c-3g-docs.googleusercontent.com (doc-0c-3g-docs.googleusercontent.com)... 74.125.195.132, 2607:f8b0:400e:c09::84\n",
            "Connecting to doc-0c-3g-docs.googleusercontent.com (doc-0c-3g-docs.googleusercontent.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 297352 (290K) [text/csv]\n",
            "Saving to: ‘test.csv’\n",
            "\n",
            "test.csv            100%[===================>] 290.38K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2021-03-02 13:49:08 (144 MB/s) - ‘test.csv’ saved [297352/297352]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CylH-J33qTou"
      },
      "source": [
        "# Training and Evaluation Helpers\r\n",
        "Here we have the helper functions that define the training and evaluation cycle, with specialised functions for models which require inputs other than the traditional sentence + grade input (e.g. Bert)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TCwEEwS3BQd"
      },
      "source": [
        "# We define our training loop\n",
        "def train(train_iter, dev_iter, model, number_epoch, loss_fn, optimizer, freeze=None):\n",
        "  \"\"\"\n",
        "  Training loop for the model, which calls on eval to evaluate after each epoch\n",
        "  \"\"\"\n",
        "  training_stats = []\n",
        "  print(\"Training model.\")\n",
        "\n",
        "  frozen = False\n",
        "  if (freeze != None):\n",
        "    assert (freeze > 0 and number_epoch >= freeze)\n",
        "    model.freeze_embeddings()    \n",
        "    frozen = True\n",
        "\n",
        "  for epoch in range(1, number_epoch+1):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_sse = 0\n",
        "    no_observations = 0  # Observations used for training so far\n",
        "\n",
        "    if (frozen and epoch > freeze):\n",
        "      model.unfreeze_embeddings()\n",
        "      frozen = False\n",
        "\n",
        "    for feature, target in train_iter:\n",
        "      # for RNN:\n",
        "      model.batch_size = target.shape[0]\n",
        "      no_observations = no_observations + target.shape[0]\n",
        "      model.hidden = model.init_hidden()\n",
        "\n",
        "      if torch.is_tensor(feature):\n",
        "        feature = feature.to(device)\n",
        "      if torch.is_tensor(target):\n",
        "        target  = target.to(device)\n",
        "\n",
        "      predictions = model(feature).squeeze(1)\n",
        "      optimizer.zero_grad()\n",
        "      loss = loss_fn(predictions, target)\n",
        "      sse, __ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy())\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_loss += loss.item()*target.shape[0]\n",
        "      epoch_sse += sse\n",
        "\n",
        "      loss = loss.detach().cpu().item()\n",
        "      del loss\n",
        "      gc.collect()\n",
        "\n",
        "    valid_loss, valid_mse, __, __ = eval(dev_iter, model, loss_fn)\n",
        "    epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\n",
        "\n",
        "    print('| Epoch: %.2d | Train Loss: %.4f | Train RMSE: %.4f | Val. Loss: %.4f | Val. RMSE: %.4f |' % (\n",
        "      epoch, epoch_loss, epoch_mse**0.5, valid_loss, valid_mse**0.5\n",
        "    ))\n",
        "\n",
        "    training_stats.append({\n",
        "            'epoch': epoch,\n",
        "            'train_loss': epoch_loss,\n",
        "            'train_rmse': epoch_mse**0.5,\n",
        "            'val_loss': valid_loss,\n",
        "            'val_rmse': valid_mse**0.5\n",
        "    })\n",
        "\n",
        "  return training_stats"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zIJcnrbRzhg"
      },
      "source": [
        "# Used to get predictions for evaluation\r\n",
        "def get_predictions(test_iter, model):\r\n",
        "  \"\"\"\r\n",
        "  Training loop for the model, which calls on eval to evaluate after each epoch\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  preds = np.array([])\r\n",
        "  pred_ids = np.array([])\r\n",
        "  model.eval()\r\n",
        "  for feature, ids in test_iter:\r\n",
        "    # for RNN:\r\n",
        "    model.batch_size = ids.shape[0]\r\n",
        "    model.hidden = model.init_hidden()\r\n",
        "\r\n",
        "    if torch.is_tensor(feature):\r\n",
        "        feature = feature.to(device)\r\n",
        "    predictions = model(feature).squeeze(1)\r\n",
        "    preds = np.concatenate((preds, predictions.detach().cpu().numpy()))\r\n",
        "    pred_ids = np.concatenate((pred_ids, ids.detach().cpu().numpy()))\r\n",
        "\r\n",
        "  return np.array(preds), np.array(pred_ids)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJeBxYW93BQd"
      },
      "source": [
        "# We evaluate performance on our dev set\n",
        "def eval(data_iter, model, loss_fn):\n",
        "  \"\"\"\n",
        "  Evaluating model performance on the dev set\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "  epoch_sse = 0\n",
        "  pred_all = []\n",
        "  trg_all = []\n",
        "  no_observations = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in data_iter:\n",
        "      feature, target = batch\n",
        "      if torch.is_tensor(feature):\n",
        "        feature = feature.to(device)\n",
        "      if torch.is_tensor(target):\n",
        "        target  = target.to(device)\n",
        "\n",
        "      # for RNN:\n",
        "      model.batch_size = target.shape[0]\n",
        "      no_observations = no_observations + target.shape[0]\n",
        "      model.hidden = model.init_hidden()\n",
        "\n",
        "      predictions = model(feature).squeeze(1)\n",
        "      loss = loss_fn(predictions, target)\n",
        "\n",
        "      # We get the mse\n",
        "      pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n",
        "      sse, __ = model_performance(pred, trg)\n",
        "\n",
        "      epoch_loss += loss.item()*target.shape[0]\n",
        "      epoch_sse += sse\n",
        "      pred_all.extend(pred)\n",
        "      trg_all.extend(trg)\n",
        "\n",
        "  return epoch_loss/no_observations, epoch_sse/no_observations, np.array(pred_all), np.array(trg_all)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO4PMTUl3BQd"
      },
      "source": [
        "# How we print the model performance\n",
        "def model_performance(output, target, print_output=False):\n",
        "  \"\"\"\n",
        "  Returns SSE and MSE per batch (printing the MSE and the RMSE)\n",
        "  \"\"\"\n",
        "  sq_error = (output - target)**2\n",
        "  sse = np.sum(sq_error)\n",
        "  mse = np.mean(sq_error)\n",
        "  rmse = np.sqrt(mse)\n",
        "\n",
        "  if print_output:\n",
        "    print(f'| MSE: {mse:.2f} | RMSE: {rmse:.2f} |')\n",
        "\n",
        "  return sse, mse"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAFZTY7WZ2hY"
      },
      "source": [
        "from gensim.models import Word2Vec\r\n",
        "import multiprocessing\r\n",
        "\r\n",
        "# Train our own word embedding\r\n",
        "def train_word_embedding(sentences, e_dim=100, window=5, negative=15, iter=10, workers=multiprocessing.cpu_count()):\r\n",
        "  return Word2Vec(sentences, size=e_dim, window=5, negative=15, iter=10, workers=multiprocessing.cpu_count())"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU9eB_qn0q-e"
      },
      "source": [
        "def score_task_1(truth_loc, prediction_loc):\r\n",
        "    truth = pd.read_csv(truth_loc, usecols=['id','meanGrade'])\r\n",
        "    pred = pd.read_csv(prediction_loc, usecols=['id','pred'])\r\n",
        "\r\n",
        "    assert(sorted(truth.id) == sorted(pred.id)),\"ID mismatch between ground truth and prediction!\"\r\n",
        "    \r\n",
        "    data = pd.merge(truth,pred)\r\n",
        "    rmse = np.sqrt(np.mean((data['meanGrade'] - data['pred'])**2))\r\n",
        "    \r\n",
        "    print(\"RMSE = %.3f\" % rmse)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UWJ9Z96uay3"
      },
      "source": [
        "# Helper Functions\r\n",
        "\r\n",
        "Helper functions for preprocessing or during model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkYFRpHt3BQe"
      },
      "source": [
        "def create_vocab(data):\n",
        "  \"\"\"\n",
        "  Creating a corpus of all the tokens used\n",
        "  \"\"\"\n",
        "  tokenized_corpus = [] # Let us put the tokenized corpus in a list\n",
        "\n",
        "  for sentence in data:\n",
        "    tokenized_sentence = []\n",
        "\n",
        "    for token in sentence.split(' '): # simplest split is\n",
        "      tokenized_sentence.append(token)\n",
        "\n",
        "    tokenized_corpus.append(tokenized_sentence)\n",
        "\n",
        "  # Create single list of all vocabulary\n",
        "  vocabulary = []  # Let us put all the tokens (mostly words) appearing in the vocabulary in a list\n",
        "\n",
        "  for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "      if token not in vocabulary:\n",
        "          vocabulary.append(token)\n",
        "\n",
        "  return vocabulary, tokenized_corpus"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu_7IziNqAPQ"
      },
      "source": [
        "def get_sentences(df, og_label='original', edit_label='edit'):\r\n",
        "  \"\"\"\r\n",
        "  Extract the original and new sentences + words from a dataframe\r\n",
        "  \"\"\"\r\n",
        "  p = r\"<(.*)\\/>\"\r\n",
        "  replace_regex = re.compile(p)\r\n",
        "  og_word = []\r\n",
        "  new_word = []\r\n",
        "  og_sentences = []\r\n",
        "  new_sentences = []\r\n",
        "\r\n",
        "  for s, w in df[[og_label, edit_label]].itertuples(index=False,name=None):\r\n",
        "    tokens = s.split(' ') # For each sentence get the words\r\n",
        "    m = replace_regex.search(str(s)) # Get the word to replace\r\n",
        "\r\n",
        "    assert not m is None # Couldn't regex match the replacement word\r\n",
        "\r\n",
        "    og_word.append(m.group(1))\r\n",
        "    new_word.append(w)\r\n",
        "    og_sentences.append(replace_regex.sub( m.group(1), s))\r\n",
        "    new_sentences.append(replace_regex.sub(w, s))\r\n",
        "  \r\n",
        "  return og_sentences, new_sentences, og_word, new_word\r\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_xUhSV43BQe"
      },
      "source": [
        "def softmax_mask(batch, mask):\n",
        "    normalizing_mask = torch.Tensor([[float('-inf') if token == 0 else 0 for token in entry] for entry in mask]).to(device)\n",
        "    return torch.nn.functional.softmax(batch + normalizing_mask, dim=-1)\n",
        "\n",
        "def padd_mask(batch):\n",
        "    return torch.Tensor([[0 if token == 0 else 1 for token in entry] for entry in batch]).to(device)\n",
        "\n",
        "def collate_fn_padd(batch):\n",
        "  '''\n",
        "  We add padding to our minibatches and create tensors for our model\n",
        "  '''\n",
        "  batch_labels = [l for f, l in batch]\n",
        "  batch_features = [f for f, l in batch]\n",
        "  batch_features_len = [len(f) for f, l in batch]\n",
        "  seq_tensor = torch.zeros((len(batch), max(batch_features_len))).long()\n",
        "\n",
        "  for idx, (seq, seqlen) in enumerate(zip(batch_features, batch_features_len)):\n",
        "    seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
        "\n",
        "  batch_labels = torch.FloatTensor(batch_labels)\n",
        "  return seq_tensor, batch_labels\n",
        "\n",
        "class Task1Dataset(Dataset):\n",
        "  def __init__(self, train_data, labels):\n",
        "    self.x_train = train_data\n",
        "    self.y_train = labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.y_train)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    return self.x_train[item], self.y_train[item]\n",
        "\n",
        "def collate_fn_padd_features(batch):\n",
        "  '''\n",
        "  We add padding to our minibatches and create tensors for our model\n",
        "  '''\n",
        "  batch_features = [f for f in batch]\n",
        "  batch_features_len = [len(f) for f in batch]\n",
        "  seq_tensor = torch.zeros((len(batch), max(batch_features_len)), dtype=torch.long)\n",
        "\n",
        "  for idx, (seq, seqlen) in enumerate(zip(batch_features, batch_features_len)):\n",
        "    seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
        "\n",
        "  return seq_tensor\n",
        "\n",
        "def collate_fn_padd_labels(batch):\n",
        "  '''\n",
        "  We add padding to our minibatches and create tensors for our model\n",
        "  '''\n",
        "  sequences = [f for f, l in batch]\n",
        "  batch_labels = torch.FloatTensor([l for f, l in batch])\n",
        "  return sequences, batch_labels"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6I-3E_ev3sd"
      },
      "source": [
        "def encode_edited(edited_sentence, grades, tokenizer):\r\n",
        "  encoded_data = []\r\n",
        "  attention_data = []\r\n",
        "\r\n",
        "  for sentence in edited_sentence:\r\n",
        "    encoded = tokenizer.encode_plus(sentence)\r\n",
        "    encoded_data.append(encoded['input_ids'])\r\n",
        "\r\n",
        "  # Split train dataset to train and validation sets\r\n",
        "  train_val_dataset = Task1Dataset(encoded_data, grades)\r\n",
        "\r\n",
        "  num_train = round(len(train_val_dataset) * train_proportion)\r\n",
        "  num_val = len(train_val_dataset) - num_train\r\n",
        "  train_dataset, val_dataset = random_split(train_val_dataset, (num_train, num_val))\r\n",
        "\r\n",
        "  # Create dataloaders from the tokenised embeddings\r\n",
        "  train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, collate_fn=collate_fn_padd)\r\n",
        "  val_dataloader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn_padd)\r\n",
        "\r\n",
        "  return train_dataloader, val_dataloader\r\n",
        "\r\n",
        "def encode_both(og_sentence, edited_sentence, grades, tokenizer):\r\n",
        "  encoded_data = []\r\n",
        "  attention_data = []\r\n",
        "\r\n",
        "  for og, new in zip(og_sentence, edited_sentence):\r\n",
        "    encoded = tokenizer.encode_plus(og, text_pair=new)\r\n",
        "    encoded_data.append(encoded['input_ids'])\r\n",
        "\r\n",
        "  # Split train dataset to train and validation sets\r\n",
        "  train_val_dataset = Task1Dataset(encoded_data, grades)\r\n",
        "\r\n",
        "  num_train = round(len(train_val_dataset) * train_proportion)\r\n",
        "  num_val = len(train_val_dataset) - num_train\r\n",
        "  train_dataset, val_dataset = random_split(train_val_dataset, (num_train, num_val))\r\n",
        "\r\n",
        "  # Create dataloaders from the tokenised embeddings\r\n",
        "  train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, collate_fn=collate_fn_padd)\r\n",
        "  val_dataloader = DataLoader(val_dataset, shuffle=True, batch_size=batch_size, collate_fn=collate_fn_padd)\r\n",
        "\r\n",
        "  return train_dataloader, val_dataloader"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGu9dgOTPTUa"
      },
      "source": [
        "# Get average word length of edited sentence.\r\n",
        "def avg_word(sentence):\r\n",
        "  words = sentence.split()\r\n",
        "  return (sum(len(word) for word in words)/len(words))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJuXczVTryAW"
      },
      "source": [
        "# Creates the vectorized sequences from our training corpus data\r\n",
        "def get_word_vecs_dicts(joint_vocab, train_tokenized_corpus, word_vec=None, filename=None):\r\n",
        "  # Either word_vec or filename must be defined\r\n",
        "  assert not (filename is None and word_vec is None)\r\n",
        "\r\n",
        "  wvecs = []\r\n",
        "  word2idx = []\r\n",
        "  idx2word = []\r\n",
        "  index = 1\r\n",
        "\r\n",
        "  if filename is None:\r\n",
        "    for word in joint_vocab:\r\n",
        "      if word in word_vec.vocab:\r\n",
        "        vec = word_vec[word]\r\n",
        "        wvecs.append(vec)\r\n",
        "        word2idx.append((word, index))\r\n",
        "        idx2word.append((index, word))\r\n",
        "        index += 1\r\n",
        "  else:\r\n",
        "    with codecs.open(filename, 'r','utf-8') as f:\r\n",
        "      for line in f.readlines():\r\n",
        "        # Ignore the first line - first line typically contains vocab, dimensionality\r\n",
        "        if len(line.strip().split()) > 3:\r\n",
        "          word = line.strip().split()[0]\r\n",
        "          if word in joint_vocab:\r\n",
        "            (word, vec) = (word, list(map(float, line.strip().split()[1:])))\r\n",
        "            wvecs.append(vec)\r\n",
        "            word2idx.append((word, index))\r\n",
        "            idx2word.append((index, word))\r\n",
        "            index += 1\r\n",
        "\r\n",
        "  wvecs = np.array(wvecs)\r\n",
        "  word2idx = dict(word2idx)\r\n",
        "  idx2word = dict(idx2word)\r\n",
        "  cur_len = len(word2idx)\r\n",
        "  vectorized_seqs = [[word2idx[tok] for tok in seq if tok in word2idx] for seq in training_tokenized_corpus]\r\n",
        "\r\n",
        "  # To avoid any sentences being empty (if no words match to our word embeddings)\r\n",
        "  vectorized_seqs = [x if len(x) > 0 else [0] for x in vectorized_seqs]\r\n",
        "\r\n",
        "  return wvecs, vectorized_seqs, cur_len"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EjaYNSlle2h"
      },
      "source": [
        "def encode_wvecs(vec_seqs, grades):\r\n",
        "  # Create the dataset from vectorised sequences\r\n",
        "  train_and_val = Task1Dataset(vec_seqs, grades)\r\n",
        "\r\n",
        "  # Split based on the train_proportion hyperparameter\r\n",
        "  train_examples = round(len(train_and_val) * train_proportion)\r\n",
        "  val_examples = len(train_and_val) - train_examples\r\n",
        "  train_dataset, val_dataset = random_split(train_and_val, (train_examples, val_examples))\r\n",
        "\r\n",
        "  train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=batch_size, collate_fn=collate_fn_padd)\r\n",
        "  val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn_padd)\r\n",
        "\r\n",
        "  return train_loader, val_loader"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdC_r9kcbqnZ"
      },
      "source": [
        "# Models\r\n",
        "\r\n",
        "Below are the model architectures that was at some point part of the testing of the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9nQpept3BQg"
      },
      "source": [
        "# Skeleton BiLSTM model\n",
        "class BiLSTM(nn.Module):\n",
        "  def __init__(self, embedding_dim, hidden_dim, vocab_size, batch_size, device):\n",
        "    super(BiLSTM, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.device = device\n",
        "    self.batch_size = batch_size\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "    # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "    # with dimensionality hidden_dim.\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
        "\n",
        "    # The linear layer that maps from hidden state space to tag space\n",
        "    self.hidden2label = nn.Linear(hidden_dim * 2, 1)\n",
        "    self.hidden = self.init_hidden()\n",
        "\n",
        "  def init_hidden(self):\n",
        "    # Before we've done anything, we dont have any hidden state.\n",
        "    # Refer to the Pytorch documentation to see exactly why they have this dimensionality.\n",
        "    # The axes semantics are (num_layers * num_directions, minibatch_size, hidden_dim)\n",
        "    return torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device), \\\n",
        "            torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device)\n",
        "\n",
        "  def forward(self, sentence):\n",
        "    embedded = self.embedding(sentence)\n",
        "    embedded = embedded.permute(1, 0, 2)\n",
        "\n",
        "    lstm_out, self.hidden = self.lstm(\n",
        "        embedded.view(len(embedded), self.batch_size, self.embedding_dim), self.hidden)\n",
        "\n",
        "    out = self.hidden2label(lstm_out[-1])\n",
        "    return out"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaRo-Ma1b62N"
      },
      "source": [
        "# RoBERTa\r\n",
        "class RoBERTa(nn.Module):\r\n",
        "  def __init__(self, embedding_dim, hidden_dim, batch_size, device, \r\n",
        "               roberta_pretrained='roberta-base', output_hidden=False):\r\n",
        "    super(RoBERTa, self).__init__()\r\n",
        "    self.hidden_dim = hidden_dim\r\n",
        "    self.embedding_dim = embedding_dim\r\n",
        "    self.device = device\r\n",
        "    self.batch_size = batch_size\r\n",
        "    self.roberta = RobertaModel.from_pretrained(roberta_pretrained)\r\n",
        "    self.output_hidden = output_hidden\r\n",
        "    self.output_len = hidden_dim * 2 if output_hidden else 1\r\n",
        "\r\n",
        "    # The LSTM takes word embeddings as inputs, and outputs hidden states with hidden_dim dimensions\r\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\r\n",
        "    self.hidden2label = nn.Linear(hidden_dim * 2, 1)\r\n",
        "    self.hidden = self.init_hidden()\r\n",
        "\r\n",
        "    # Attention Layer?\r\n",
        "    self.attn_linear = nn.Linear(hidden_dim * 2, 1)\r\n",
        "\r\n",
        "  def freeze_embeddings(self):\r\n",
        "        for param in self.roberta.base_model.parameters():\r\n",
        "            param.requires_grad = False\r\n",
        "\r\n",
        "  def unfreeze_embeddings(self):\r\n",
        "      for param in self.roberta.base_model.parameters():\r\n",
        "          param.requires_grad = True\r\n",
        "\r\n",
        "  def init_hidden(self):\r\n",
        "    # The axes semantics are (num_layers * num_directions, minibatch_size, hidden_dim)\r\n",
        "    return torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device), \\\r\n",
        "            torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device)\r\n",
        "\r\n",
        "  # Run this or directly define the tokenizer attribute\r\n",
        "  def init_preprocess(self, tokenizer):\r\n",
        "    self.tokenizer = tokenizer\r\n",
        "\r\n",
        "  # Used for multimodel sentence parsing\r\n",
        "  def preprocess(self):\r\n",
        "    \r\n",
        "    # Assume sentences come in the form of (batch_size x sentences)\r\n",
        "    def encode_batch(sentences):\r\n",
        "      return collate_fn_padd_features(list(map(self.tokenizer.encode, sentences))).to(device)\r\n",
        "\r\n",
        "    return encode_batch\r\n",
        "\r\n",
        "  def get_embedded(self, sentence, grad=True):\r\n",
        "    mask = (sentence != 0).type(torch.long)\r\n",
        "    # Run the text through BERT, and collect all of the hidden states produced from all 12 layers\r\n",
        "    if grad:\r\n",
        "      # The last hidden-state is the first element of the output tuple\r\n",
        "      out = self.roberta(sentence, mask)\r\n",
        "      if 'last_hidden_state' in out:\r\n",
        "        last_hidden_states = out['last_hidden_state']\r\n",
        "      elif 'hidden_states' in out:\r\n",
        "        last_hidden_states = out['hidden_states'][-1]\r\n",
        "      else:\r\n",
        "        # Couldn't extract hidden state\r\n",
        "        assert False\r\n",
        "    else:\r\n",
        "      with torch.no_grad():\r\n",
        "        out = self.roberta(sentence, mask)\r\n",
        "        if 'last_hidden_state' in out:\r\n",
        "          last_hidden_states = out['last_hidden_state']\r\n",
        "        elif 'hidden_states' in out:\r\n",
        "          last_hidden_states = out['hidden_states'][-1]\r\n",
        "        else:\r\n",
        "          # Couldn't extract hidden state\r\n",
        "          assert False\r\n",
        "\r\n",
        "    return last_hidden_states.type(torch.float32)\r\n",
        "\r\n",
        "  def forward(self, sentence):\r\n",
        "    mask = (sentence != 0).type(torch.long)\r\n",
        "    # Get word embeddings. BERT base gives us 768 hidden parameters for each word.\r\n",
        "    embedded = self.get_embedded(sentence) # (batch_size, max_len)\r\n",
        "\r\n",
        "    # (max_len, batch_size, 768) -> (batch_size, max_len, directions * hidden_dim)\r\n",
        "    lstm_out, self.hidden = self.lstm(embedded, self.hidden)\r\n",
        "\r\n",
        "    # Attention Mechanism\r\n",
        "    # Get similarity using DOT attention (i think)\r\n",
        "    # (batch_size, max_len, directions * hidden_dim) -> (batch_size, max_len)\r\n",
        "    att_out = self.attn_linear(lstm_out).squeeze(-1)\r\n",
        "    # Get the attention weights for each token in a sentence (batch_size)\r\n",
        "    # (batch_size, max_len) -> (batch_size, max_len)\r\n",
        "    # att_out = torch.nn.functional.softmax(att_out, dim=-1)\r\n",
        "    att_out = softmax_mask(att_out, mask)\r\n",
        "    # Get sentence vector which is a weighted sum of token hidden states\r\n",
        "    # (batch_size, max_len) -> (batch_size, directions * hidden_dim)\r\n",
        "    att_out = torch.sum(att_out.unsqueeze(-1) * lstm_out, dim=1)\r\n",
        "\r\n",
        "    # out = self.hidden2label(lstm_out[-1])\r\n",
        "    return att_out if self.output_hidden else self.hidden2label(att_out)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z94C03CgV7h"
      },
      "source": [
        "# Bert\r\n",
        "class BERT(nn.Module):\r\n",
        "  def __init__(self, embedding_dim, hidden_dim, batch_size, device, \r\n",
        "               bert_model=None, bert_pretrained='bert-base-uncased', output_hidden=False):\r\n",
        "    super(BERT, self).__init__()\r\n",
        "    self.hidden_dim = hidden_dim\r\n",
        "    self.embedding_dim = embedding_dim\r\n",
        "    self.device = device\r\n",
        "    self.batch_size = batch_size\r\n",
        "    self.bert = BertModel.from_pretrained(bert_pretrained).to(device) if bert_model == None else bert_model\r\n",
        "    self.output_hidden = output_hidden\r\n",
        "    self.output_len = hidden_dim * 2 if output_hidden else 1\r\n",
        "\r\n",
        "    # The LSTM takes word embeddings as inputs, and outputs hidden states with hidden_dim dimensions\r\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\r\n",
        "    self.hidden2label = nn.Linear(hidden_dim * 2, 1)\r\n",
        "    self.hidden = self.init_hidden()\r\n",
        "\r\n",
        "    # Attention Layer?\r\n",
        "    self.attn_linear = nn.Linear(hidden_dim * 2, 1)\r\n",
        "\r\n",
        "  def freeze_embeddings(self):\r\n",
        "        for param in self.bert.base_model.parameters():\r\n",
        "            param.requires_grad = False\r\n",
        "\r\n",
        "  def unfreeze_embeddings(self):\r\n",
        "      for param in self.bert.base_model.parameters():\r\n",
        "          param.requires_grad = True\r\n",
        "\r\n",
        "  def init_hidden(self):\r\n",
        "    # The axes semantics are (num_layers * num_directions, minibatch_size, hidden_dim)\r\n",
        "    return torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device), \\\r\n",
        "            torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device)\r\n",
        "\r\n",
        "  # Run this or directly define the tokenizer attribute\r\n",
        "  def init_preprocess(self, tokenizer):\r\n",
        "    self.tokenizer = tokenizer\r\n",
        "\r\n",
        "  # Used for multimodel sentence parsing\r\n",
        "  def preprocess(self):\r\n",
        "    \r\n",
        "    # Assume sentences come in the form of (batch_size x sentences)\r\n",
        "    def encode_batch(sentences):\r\n",
        "      return collate_fn_padd_features(list(map(self.tokenizer.encode, sentences))).to(device)\r\n",
        "\r\n",
        "    return encode_batch\r\n",
        "\r\n",
        "  def get_embedded(self, sentence, grad=True):\r\n",
        "    mask = (sentence != 0).type(torch.long)\r\n",
        "    # Run the text through BERT, and collect all of the hidden states produced from all 12 layers\r\n",
        "    if grad:\r\n",
        "      # The last hidden-state is the first element of the output tuple\r\n",
        "      out = self.bert(sentence, mask)\r\n",
        "      if 'last_hidden_state' in out:\r\n",
        "        last_hidden_states = out['last_hidden_state']\r\n",
        "      elif 'hidden_states' in out:\r\n",
        "        last_hidden_states = out['hidden_states'][-1]\r\n",
        "      else:\r\n",
        "        # Couldn't extract hidden state\r\n",
        "        assert False\r\n",
        "    else:\r\n",
        "      with torch.no_grad():\r\n",
        "        out = self.bert(sentence, mask)\r\n",
        "        if 'last_hidden_state' in out:\r\n",
        "          last_hidden_states = out['last_hidden_state']\r\n",
        "        elif 'hidden_states' in out:\r\n",
        "          last_hidden_states = out['hidden_states'][-1]\r\n",
        "        else:\r\n",
        "          # Couldn't extract hidden state\r\n",
        "          assert False\r\n",
        "\r\n",
        "    return last_hidden_states.type(torch.float32)\r\n",
        "\r\n",
        "  def forward(self, sentence):\r\n",
        "    mask = (sentence != 0).type(torch.long)\r\n",
        "    # Get word embeddings. BERT base gives us 768 hidden parameters for each word.\r\n",
        "    embedded = self.get_embedded(sentence) # (batch_size, max_len)\r\n",
        "\r\n",
        "    # (max_len, batch_size, 768) -> (batch_size, max_len, directions * hidden_dim)\r\n",
        "    lstm_out, self.hidden = self.lstm(embedded, self.hidden)\r\n",
        "\r\n",
        "    # Attention Mechanism\r\n",
        "    # Get similarity using DOT attention (i think)\r\n",
        "    # (batch_size, max_len, directions * hidden_dim) -> (batch_size, max_len)\r\n",
        "    att_out = self.attn_linear(lstm_out).squeeze(-1)\r\n",
        "    # Get the attention weights for each token in a sentence (batch_size)\r\n",
        "    # (batch_size, max_len) -> (batch_size, max_len)\r\n",
        "    # att_out = torch.nn.functional.softmax(att_out, dim=-1)\r\n",
        "    att_out = softmax_mask(att_out, mask)\r\n",
        "    # Get sentence vector which is a weighted sum of token hidden states\r\n",
        "    # (batch_size, max_len) -> (batch_size, directions * hidden_dim)\r\n",
        "    att_out = torch.sum(att_out.unsqueeze(-1) * lstm_out, dim=1)\r\n",
        "    \r\n",
        "    # out = self.hidden2label(lstm_out[-1])\r\n",
        "    return att_out if self.output_hidden else self.hidden2label(att_out)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODOdGheoeksf"
      },
      "source": [
        "# Parallel Multi-model implementation\r\n",
        "class MultiModel(nn.Module):\r\n",
        "  def __init__(self, models, hidden_inits, batch_size, output_dims):\r\n",
        "    '''\r\n",
        "    A Wrapper which takes multiple models and gets their concatenated linearly transformed results\r\n",
        "\r\n",
        "    Inputs:\r\n",
        "    models (dict) -> key : model name (string), values : (nn.Module) model (with forward)\r\n",
        "    hidden_inits (list) -> List of model names where init_hidden is required\r\n",
        "    preprocess (dict) -> key : model name (string), values : (string -> token array) model's encoder for the sentence\r\n",
        "    '''\r\n",
        "    super(MultiModel, self).__init__()\r\n",
        "    self.models = models\r\n",
        "    self.hidden_inits = hidden_inits\r\n",
        "    self.linear2label = nn.Linear(output_dims, 1)\r\n",
        "    self.batch_size = batch_size\r\n",
        "\r\n",
        "    # Requires the preprocess function which tokenizes a raw sentence to tokens\r\n",
        "    for m in models.values():\r\n",
        "      assert hasattr(m, 'preprocess')\r\n",
        "\r\n",
        "  # For RNNs, init hiddens\r\n",
        "  def init_hidden(self):\r\n",
        "    for m in self.hidden_inits:\r\n",
        "      self.models[m].batch_size = self.batch_size\r\n",
        "      self.models[m].hidden = self.models[m].init_hidden()\r\n",
        "\r\n",
        "  def forward(self, sentence):\r\n",
        "    out_cat = []\r\n",
        "\r\n",
        "    for m in self.models.values():\r\n",
        "      encoded = m.preprocess()(sentence)\r\n",
        "      preds = m(encoded)\r\n",
        "      out_cat.append(preds)\r\n",
        "\r\n",
        "    out = torch.cat(out_cat, dim=1)\r\n",
        "    out = self.linear2label(out)\r\n",
        "    return out"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnJse9uFu8UH"
      },
      "source": [
        "# Initialisation + Preprocessing\r\n",
        "Prepare the data for training, assign hyperparameters, initialise the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNTcGaIaJ63d"
      },
      "source": [
        "# Hyperparameters\r\n",
        "bert_type = 'bert-base-uncased'\r\n",
        "rob_type  = 'roberta-base'\r\n",
        "pad_len = 64\r\n",
        "\r\n",
        "e_dim = 768\r\n",
        "h_dim = 50\r\n",
        "\r\n",
        "batch_size = 32\r\n",
        "train_proportion = 0.8"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5xb_8Xufqqx"
      },
      "source": [
        "#### Extract and parse the edited words and sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GWcu5MufqLa"
      },
      "source": [
        "# Parse training and test data to tuple of original sentences, new sentences, etc.\r\n",
        "x_og, x_new, x_og_word, x_new_word = get_sentences(train_df)\r\n",
        "y_og, y_new, y_og_word, y_new_word = get_sentences(test_df)\r\n",
        "x_grades = train_df['meanGrade']\r\n",
        "\r\n",
        "# Insert these new sentences into the dataframe\r\n",
        "train_df['editedSentence'] = x_new\r\n",
        "train_df['replaceWord'] = x_og_word\r\n",
        "test_df['editedSentence'] = y_new\r\n",
        "test_df['replaceWord'] = y_og_word"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUKGivqVr7kA"
      },
      "source": [
        "### Prepare Bert and RoBerta specfic Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPzQbQazzoey"
      },
      "source": [
        "# Get Tokenizer for preprocessing\r\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(bert_type, do_lower_case=True)\r\n",
        "rob_tokenizer = RobertaTokenizer.from_pretrained(rob_type, do_lower_case=True)\r\n",
        "\r\n",
        "# Get the dataloaders\r\n",
        "bert_single_train_loader, bert_single_val_loader = encode_edited(x_new, x_grades, bert_tokenizer)\r\n",
        "bert_both_train_loader, bert_both_val_loader = encode_both(x_og, x_new, x_grades, bert_tokenizer)\r\n",
        "robert_single_train_loader, robert_single_val_loader = encode_edited(x_new, x_grades, rob_tokenizer)\r\n",
        "robert_both_train_loader, robert_both_val_loader = encode_both(x_og, x_new, x_grades, rob_tokenizer)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r87Ra9hmO0MT"
      },
      "source": [
        "### Prepare Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPgvPodJPJLh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d09c4f58-0c6a-4eae-ea5b-df918eceb513"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "from nltk.corpus import stopwords\r\n",
        "stop = stopwords.words('english')\r\n",
        "\r\n",
        "def avg_word(sentence):\r\n",
        "  words = sentence.split()\r\n",
        "  return (sum(len(word) for word in words)/len(words))\r\n",
        "\r\n",
        "# This creates a new dataframe with the following extracted features.\r\n",
        "def add_extracted_features(orig_df):\r\n",
        "  # Create new dataframe.\r\n",
        "  train_df = orig_df.copy(deep=True)\r\n",
        " \r\n",
        "  # Average word length of edited sentence.\r\n",
        "  train_df['avgWordLength'] = train_df['editedSentence'].apply(lambda x: avg_word(x))\r\n",
        "\r\n",
        "  # Word counts per sentence.\r\n",
        "  train_df['wordCounts'] = train_df['editedSentence'].apply(lambda x: len(str(x).split(\" \")))\r\n",
        "  \r\n",
        "  # Character counts per edited sentence.\r\n",
        "  train_df['charCounts'] = train_df['editedSentence'].str.len()  # This also includes spaces.\r\n",
        "\r\n",
        "  # Number of stop words per edited sentence.\r\n",
        "  train_df['numStopWords'] = train_df['editedSentence'].apply(lambda x: len([x for x in x.split() if x in stop]))\r\n",
        "\r\n",
        "  # Number of upper case words.\r\n",
        "  train_df['editedSentence'].apply(lambda x: len([x for x in x.split() if x.isupper()])).value_counts()\r\n",
        "  train_df['numUpperCase'] = train_df['editedSentence'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\r\n",
        "\r\n",
        "  # Number of sentences with digits.\r\n",
        "  train_df['editedSentence'].apply(lambda x: len([x for x in x.split() if x.isdigit()])).value_counts()\r\n",
        "  train_df['numDigits'] = train_df['editedSentence'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\r\n",
        "\r\n",
        "  # Number of sentences with hashtags.\r\n",
        "  train_df['editedSentence'].apply(lambda x: len([x for x in x.split() if x.startswith('#')])).value_counts()\r\n",
        "  train_df['numHashtags'] = train_df['editedSentence'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\r\n",
        "\r\n",
        "  return train_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZKxnuVU50oh"
      },
      "source": [
        "#### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIVzX3eS53Hi"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "\n",
        "def get_tfidf(sentences):\n",
        "  count_vect = CountVectorizer(stop_words='english')\n",
        "  train_counts_sentence = count_vect.fit_transform(sentences)\n",
        "  transformer = TfidfTransformer().fit(train_counts_sentence)\n",
        "  train_counts_sentence = transformer.transform(train_counts_sentence)\n",
        "  return train_counts_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYJukx7v564F"
      },
      "source": [
        "#### BOW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaZRVKuL58nW"
      },
      "source": [
        "def get_bow(sentences):\n",
        "  bow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\n",
        "  train_bow_sentence = bow.fit_transform(sentences)\n",
        "  return train_bow_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2f5JkcXRU0h"
      },
      "source": [
        "### Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhDACD4mSOXz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6828f6f-f75c-45eb-aad3-872b9de410ec"
      },
      "source": [
        "from textblob import Word, TextBlob\r\n",
        "nltk.download('wordnet')\r\n",
        "\r\n",
        "# This creates a new processed dataframe.\r\n",
        "def get_processed_df(orig_df):\r\n",
        "  # Convert to lower case.\r\n",
        "  processed = pd.DataFrame(orig_df['editedSentence'].apply(lambda x: \" \".join(x.lower() for x in x.split())))\r\n",
        "  processed['original'] = orig_df['original'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\r\n",
        "\r\n",
        "  # Remove punctuation.\r\n",
        "  processed['editedSentence'] = processed['editedSentence'].str.replace('[^\\w\\s]','')\r\n",
        "  processed['original'] = processed['original'].str.replace('[^\\w\\s]','')\r\n",
        "\r\n",
        "  # Remove stop words.\r\n",
        "  processed['editedSentence'] = processed['editedSentence'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\r\n",
        "  processed['original'] = processed['original'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\r\n",
        "\r\n",
        "  # Lemmatization.\r\n",
        "  processed['editedSentence'] = processed['editedSentence'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\r\n",
        "  processed['original'] = processed['original'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\r\n",
        "  \r\n",
        "  # Extract sentiment.\r\n",
        "  processed['sentiment'] = processed['editedSentence'].apply(lambda x: TextBlob(x).sentiment[0])\r\n",
        "  processed['sentimentOrig'] = processed['original'].apply(lambda x: TextBlob(x).sentiment[0])\r\n",
        "  processed['sentimentDiff'] = processed['sentiment'] - processed['sentimentOrig']\r\n",
        "\r\n",
        "  return processed"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0IuU_Ijr0cR"
      },
      "source": [
        "#### Prepare Glove Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLBuf54LaSlZ"
      },
      "source": [
        "### Word Embeddings\r\n",
        "\r\n",
        "Here we prepare our own word embeddings for approach 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBKkObMe_M-A"
      },
      "source": [
        "# Parse training and test data to tuple of original sentences, new sentences, etc.\r\n",
        "x_og, x_new, x_og_word, x_new_word = get_sentences(train_df)\r\n",
        "y_og, y_new, y_og_word, y_new_word = get_sentences(test_df)\r\n",
        "x_grades = train_df['meanGrade']\r\n",
        "\r\n",
        "# Insert these new sentences into the dataframe\r\n",
        "train_df['editedSentence'] = x_new\r\n",
        "train_df['replaceWord'] = x_og_word\r\n",
        "test_df['editedSentence'] = y_new\r\n",
        "test_df['replaceWord'] = y_og_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Rx9lOZSdaC_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3b9d261-bf31-4173-c58a-a978e70d1b10"
      },
      "source": [
        "processed_train = get_processed_df(add_extracted_features(train_df))\r\n",
        "processed_test  = get_processed_df(add_extracted_features(test_df))\r\n",
        "\r\n",
        "train_sentence = processed_train['editedSentence']\r\n",
        "test_sentence = processed_test['editedSentence']\r\n",
        "\r\n",
        "# Creating word vectors\r\n",
        "training_vocab, training_tokenized_corpus = create_vocab(train_sentence)\r\n",
        "test_vocab, test_tokenized_corpus = create_vocab(test_sentence)\r\n",
        "\r\n",
        "# Creating joint vocab from test and train:\r\n",
        "joint_vocab, joint_tokenized_corpus = create_vocab(pd.concat([train_sentence, test_sentence]))\r\n",
        "\r\n",
        "print(\"Vocab created.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5MH-ZI1bQm_"
      },
      "source": [
        "#### Brown Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doO0CS5Pa7Ji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34484c1a-f541-4469-f522-89afb866c028"
      },
      "source": [
        "from nltk.corpus import brown\r\n",
        "nltk.download('brown')\r\n",
        "\r\n",
        "brown_news_text = brown.words(categories='news')\r\n",
        "brown_sentences = brown.sents()\r\n",
        "brown_wv = train_word_embedding(brown_sentences).wv\r\n",
        "\r\n",
        "brown_e_dim = 100\r\n",
        "brown_wvecs, brown_vec_seqs, brown_input_dim = get_word_vecs_dicts(\r\n",
        "    joint_vocab, training_tokenized_corpus, word_vec=brown_wv)\r\n",
        "brown_train_loader, brown_test_loader = encode_wvecs(brown_vec_seqs, x_grades)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5RkgsHFrk2u"
      },
      "source": [
        "#### Glove Embeddings (Pretrained)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyFsRfaUslLI",
        "outputId": "7edab09f-10b7-4601-8a57-4dd31b42e8d7"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\r\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-27 14:43:43--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-02-27 14:43:43--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-02-27 14:43:43--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  1.91MB/s    in 6m 53s  \n",
            "\n",
            "2021-02-27 14:50:36 (1.99 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voXcz08BrgyH"
      },
      "source": [
        "glove_e_dim = 100\r\n",
        "glove_wvecs, glove_vec_seqs, glove_input_dim = get_word_vecs_dicts(\r\n",
        "    joint_vocab, training_tokenized_corpus, filename='glove.6B.100d.txt')\r\n",
        "glove_train_loader, glove_test_loader = encode_wvecs(glove_vec_seqs, x_grades)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kDkb9QWvIlq"
      },
      "source": [
        "# Training\r\n",
        "Initialise optimisers and start training word embedding and models!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bOlYKQAm28w"
      },
      "source": [
        "### Models (Approach 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtK_BMjN1LCu"
      },
      "source": [
        "### Testing Functions\r\n",
        "\r\n",
        "These functions load in the test dataset and tests them at the end of training, and are not used during hyperparameter tuning (we have a validation set). There are different variants depending on which preprocessing is needed (single vs double sentence)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9jdGIZyD6y8"
      },
      "source": [
        "#### BERT Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "880aPJOLTcJO"
      },
      "source": [
        "# BERT Testing\r\n",
        "def bert_single_test(model):\r\n",
        "  test_encoded_data = []\r\n",
        "  _, test_edited_sentences, _, _ = get_sentences(true_test_df)\r\n",
        "  for sentence in test_edited_sentences:\r\n",
        "    test_encoded = bert_tokenizer.encode(sentence, add_special_tokens=True)\r\n",
        "    test_encoded_data.append(test_encoded)\r\n",
        "\r\n",
        "  test_ids = true_test_df['id']\r\n",
        "  test_dataset = Task1Dataset(test_encoded_data, test_ids)\r\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn_padd)\r\n",
        "\r\n",
        "  predictions, pred_ids = get_predictions(test_dataloader, model)\r\n",
        "  pd.DataFrame({'id': pred_ids, 'pred': predictions}).to_csv(\"./predictions.csv\")\r\n",
        "  score_task_1('./test.csv', './predictions.csv')\r\n",
        "\r\n",
        "def bert_both_test(model):\r\n",
        "  test_encoded_data = []\r\n",
        "  test_original_sentences, test_edited_sentences, _, _ = get_sentences(true_test_df)\r\n",
        "  for og, edited in zip(test_original_sentences, test_edited_sentences):\r\n",
        "    test_encoded = bert_tokenizer.encode(og, text_pair=edited, add_special_tokens=True)\r\n",
        "    test_encoded_data.append(test_encoded)\r\n",
        "\r\n",
        "  test_ids = true_test_df['id']\r\n",
        "  test_dataset = Task1Dataset(test_encoded_data, test_ids)\r\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn_padd)\r\n",
        "\r\n",
        "  predictions, pred_ids = get_predictions(test_dataloader, model)\r\n",
        "  pd.DataFrame({'id': pred_ids, 'pred': predictions}).to_csv(\"./predictions.csv\")\r\n",
        "  score_task_1('./test.csv', './predictions.csv')"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0ApOgE3GXp0"
      },
      "source": [
        "#### RoBERTa Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzEJCQsD15ce"
      },
      "source": [
        "# RoBERTa Testing\r\n",
        "def roberta_single_test(model):\r\n",
        "  test_encoded_data = []\r\n",
        "  _, test_edited_sentences, _, _ = get_sentences(true_test_df)\r\n",
        "  for sentence in test_edited_sentences:\r\n",
        "    test_encoded = rob_tokenizer.encode(sentence, add_special_tokens=True)\r\n",
        "    test_encoded_data.append(test_encoded)\r\n",
        "\r\n",
        "  test_ids = true_test_df['id']\r\n",
        "  test_dataset = Task1Dataset(test_encoded_data, test_ids)\r\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn_padd)\r\n",
        "\r\n",
        "  predictions, pred_ids = get_predictions(test_dataloader, model)\r\n",
        "  pd.DataFrame({'id': pred_ids, 'pred': predictions}).to_csv(\"./predictions.csv\")\r\n",
        "  score_task_1('./test.csv', './predictions.csv')\r\n",
        "\r\n",
        "def roberta_both_test(model):\r\n",
        "  test_encoded_data = []\r\n",
        "  test_original_sentence, test_edited_sentences, _, _ = get_sentences(true_test_df)\r\n",
        "  for og, edited in zip(test_original_sentence, test_edited_sentences):\r\n",
        "    test_encoded = rob_tokenizer.encode(og, text_pair=edited, add_special_tokens=True)\r\n",
        "    test_encoded_data.append(test_encoded)\r\n",
        "\r\n",
        "  test_ids = true_test_df['id']\r\n",
        "  test_dataset = Task1Dataset(test_encoded_data, test_ids)\r\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn_padd)\r\n",
        "\r\n",
        "  predictions, pred_ids = get_predictions(test_dataloader, model)\r\n",
        "  pd.DataFrame({'id': pred_ids, 'pred': predictions}).to_csv(\"./predictions.csv\")\r\n",
        "  score_task_1('./test.csv', './predictions.csv')"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVAnQQox1lOY"
      },
      "source": [
        "#### Multimodel Testing\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPZlniZLtIO5"
      },
      "source": [
        "# Multi-Model Testing\r\n",
        "def multi_model_testing(model):\r\n",
        "  _, test_edited_sentences, _, _ = get_sentences(true_test_df)\r\n",
        "\r\n",
        "  test_ids = true_test_df['id']\r\n",
        "  test_dataset = Task1Dataset(test_edited_sentences, test_ids)\r\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn_padd_labels)\r\n",
        "\r\n",
        "  predictions, pred_ids = get_predictions(test_dataloader, model)\r\n",
        "  pd.DataFrame({'id': pred_ids, 'pred': predictions}).to_csv(\"./predictions.csv\")\r\n",
        "  score_task_1('./test.csv', './predictions.csv')"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqyrK5zRTxRs"
      },
      "source": [
        "#### Bert with Sequence Classification with Both Sentences\r\n",
        "\r\n",
        "Warning: This model is large (12GB GPU Ram) and takes 21+ minutes per epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDRPb2lEwsOr",
        "outputId": "6e2f346d-bea1-4bda-8c7b-fa76cf36a6e0"
      },
      "source": [
        "## Bert with Sequence Classification Training with Both Sentences\r\n",
        "#=====================================#\r\n",
        "# Define run specific hyperparameters #\r\n",
        "#=====================================#\r\n",
        "lr = 2e-5\r\n",
        "epochs = 2\r\n",
        "#=====================================#\r\n",
        "# Load Pretrained Bert model for double sequence training\r\n",
        "bert_double_pretrained = BertForSequenceClassification.from_pretrained(\r\n",
        "    bert_type, \r\n",
        "    num_labels = 1,\r\n",
        "    output_hidden_states = True\r\n",
        ")\r\n",
        "bert_double_pretrained = bert_double_pretrained.to(device) # Move to cuda if possible\r\n",
        "bert_double_pretrained = bert_double_pretrained.double() # Convert classifier to regressor\r\n",
        "\r\n",
        "# Initialise full model\r\n",
        "model = BERT(e_dim, h_dim, batch_size, device, bert_double_pretrained).to(device)\r\n",
        "\r\n",
        "# Optimizer and Scheduler setup\r\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\r\n",
        "scheduler = get_linear_schedule_with_warmup(\r\n",
        "    optimizer, \r\n",
        "    num_warmup_steps=0, \r\n",
        "    num_training_steps=(epochs * len(bert_both_train_loader))\r\n",
        ")\r\n",
        "\r\n",
        "print(\"Model initialised.\")\r\n",
        "\r\n",
        "# Define the loss function\r\n",
        "loss_fn = nn.MSELoss()\r\n",
        "loss_fn = loss_fn.to(device)\r\n",
        "\r\n",
        "results = train(bert_both_train_loader, bert_both_val_loader, model, epochs, loss_fn, optimizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model initialised.\n",
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.3302 | Train RMSE: 0.5746 | Val. Loss: 0.2947 | Val. RMSE: 0.5429 |\n",
            "| Epoch: 02 | Train Loss: 0.2472 | Train RMSE: 0.4972 | Val. Loss: 0.2788 | Val. RMSE: 0.5280 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU9FblaMDwot"
      },
      "source": [
        "#### Bert Training with Double Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQuaUgp0KqO1",
        "outputId": "95c601e0-4659-45e3-fd6a-fc9f6ece09fd"
      },
      "source": [
        "## Bert Training with Double Sentence\r\n",
        "#=====================================#\r\n",
        "# Define run specific hyperparameters #\r\n",
        "#=====================================#\r\n",
        "lr = 15e-6\r\n",
        "epochs = 2\r\n",
        "#=====================================#\r\n",
        "# Initialise full model\r\n",
        "model = BERT(e_dim, h_dim, batch_size, device).to(device)\r\n",
        "\r\n",
        "# Optimizer and Scheduler setup\r\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\r\n",
        "scheduler = get_linear_schedule_with_warmup(\r\n",
        "    optimizer, \r\n",
        "    num_warmup_steps=0, \r\n",
        "    num_training_steps=(epochs * len(bert_single_train_loader))\r\n",
        ")\r\n",
        "print(\"Model initialised.\")\r\n",
        "\r\n",
        "# Define the loss function\r\n",
        "loss_fn = nn.MSELoss()\r\n",
        "loss_fn = loss_fn.to(device)\r\n",
        "\r\n",
        "results = train(bert_both_train_loader, bert_both_val_loader, model, epochs, loss_fn, optimizer)\r\n",
        "bert_both_test(model)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model initialised.\n",
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.3367 | Train RMSE: 0.5803 | Val. Loss: 0.2899 | Val. RMSE: 0.5384 |\n",
            "| Epoch: 02 | Train Loss: 0.2594 | Train RMSE: 0.5093 | Val. Loss: 0.2797 | Val. RMSE: 0.5289 |\n",
            "RMSE = 0.535\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG2j9SZYD0rc"
      },
      "source": [
        "#### Bert Training with Single Sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "defpiqYCxvNN",
        "outputId": "bbbdaedf-db1a-4734-ec45-bc70234b336f"
      },
      "source": [
        "## Bert Training with Single Sentence\r\n",
        "#=====================================#\r\n",
        "# Define run specific hyperparameters #\r\n",
        "#=====================================#\r\n",
        "lr = 15e-6\r\n",
        "epochs = 2\r\n",
        "#=====================================#\r\n",
        "# Initialise full model\r\n",
        "model = BERT(e_dim, h_dim, batch_size, device).to(device)\r\n",
        "\r\n",
        "# Optimizer and Scheduler setup\r\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\r\n",
        "scheduler = get_linear_schedule_with_warmup(\r\n",
        "    optimizer, \r\n",
        "    num_warmup_steps=0, \r\n",
        "    num_training_steps=(epochs * len(bert_single_train_loader))\r\n",
        ")\r\n",
        "print(\"Model initialised.\")\r\n",
        "\r\n",
        "# Define the loss function\r\n",
        "loss_fn = nn.MSELoss()\r\n",
        "loss_fn = loss_fn.to(device)\r\n",
        "\r\n",
        "results = train(bert_single_train_loader, bert_single_val_loader, model, epochs, loss_fn, optimizer)\r\n",
        "bert_single_test(model)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model initialised.\n",
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.3347 | Train RMSE: 0.5785 | Val. Loss: 0.3003 | Val. RMSE: 0.5480 |\n",
            "| Epoch: 02 | Train Loss: 0.2607 | Train RMSE: 0.5106 | Val. Loss: 0.2789 | Val. RMSE: 0.5281 |\n",
            "RMSE = 0.531\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpxNAmGrD3bE"
      },
      "source": [
        "#### Bert Training with Single Sentence and Freezing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "So8OG9OVLW3h",
        "outputId": "07b837d7-4dbc-4ba6-e0d5-81bcca42042a"
      },
      "source": [
        "## Bert Training with Single Sentence and Freezing\r\n",
        "#=====================================#\r\n",
        "# Define run specific hyperparameters #\r\n",
        "#=====================================#\r\n",
        "lr = 2e-5\r\n",
        "epochs = 5\r\n",
        "#=====================================#\r\n",
        "# Initialise full model\r\n",
        "model = BERT(e_dim, h_dim, batch_size, device).to(device)\r\n",
        "\r\n",
        "# Optimizer and Scheduler setup\r\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\r\n",
        "scheduler = get_linear_schedule_with_warmup(\r\n",
        "    optimizer, \r\n",
        "    num_warmup_steps=0, \r\n",
        "    num_training_steps=(epochs * len(bert_single_train_loader))\r\n",
        ")\r\n",
        "print(\"Model initialised.\")\r\n",
        "\r\n",
        "# Define the loss function\r\n",
        "loss_fn = nn.MSELoss()\r\n",
        "loss_fn = loss_fn.to(device)\r\n",
        "\r\n",
        "results = train(bert_single_train_loader, bert_single_val_loader, model, epochs, loss_fn, optimizer, freeze=3)\r\n",
        "bert_single_test(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model initialised.\n",
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.7074 | Train RMSE: 0.8411 | Val. Loss: 0.3605 | Val. RMSE: 0.6004 |\n",
            "| Epoch: 02 | Train Loss: 0.3517 | Train RMSE: 0.5930 | Val. Loss: 0.3455 | Val. RMSE: 0.5878 |\n",
            "| Epoch: 03 | Train Loss: 0.3451 | Train RMSE: 0.5874 | Val. Loss: 0.3384 | Val. RMSE: 0.5817 |\n",
            "| Epoch: 04 | Train Loss: 0.3150 | Train RMSE: 0.5612 | Val. Loss: 0.2835 | Val. RMSE: 0.5324 |\n",
            "| Epoch: 05 | Train Loss: 0.2366 | Train RMSE: 0.4864 | Val. Loss: 0.2843 | Val. RMSE: 0.5332 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDrk18mNT3YE"
      },
      "source": [
        "#### RoBERTa Single"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200,
          "referenced_widgets": [
            "aadfb751523748d7a69b19fa5064d1e6",
            "918221bb80d24a4b8a676baa22ad83a6",
            "fe7b42baede0437d8559d53d9544d354",
            "fbb5745245de4178854a0e0ed227603b",
            "e2d445db538e4013b8e699f608d23315",
            "a6e7469b0dba4f49bd5becbd530c3fa8",
            "5553c6513b7e44a0938673de57f10e42",
            "43057c320f31478c884c4ef3beb88fd2",
            "e59e87581aaa43cea3fae89cfbd0bdd6",
            "6dff85f849e64a7c9a16ad5af717edf3",
            "18d17e97132c426ea3a2f112fde7695f",
            "5af7239138ec4701a2a4bdd479af07f6",
            "65bb17d000e14cfc9fc2e5afb0ea79b0",
            "a1c2272fcdfc48e2959a321ca2cfccc3",
            "fbec9a81efa44b6ebf33147737140f0b",
            "67b53d6aff6a4ca5bf196fe1491a7220"
          ]
        },
        "id": "OB0zCYH11jJp",
        "outputId": "ecb690bb-6a27-4ecc-f8ac-dc485a4186e8"
      },
      "source": [
        "## Roberta Single Training\r\n",
        "#=====================================#\r\n",
        "# Define run specific hyperparameters #\r\n",
        "#=====================================#\r\n",
        "lr = 2e-5\r\n",
        "epochs = 2\r\n",
        "#=====================================#\r\n",
        "# Initialise full model\r\n",
        "model = RoBERTa(e_dim, h_dim, batch_size, device).to(device)\r\n",
        "\r\n",
        "# Optimizer and Scheduler setup\r\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\r\n",
        "scheduler = get_linear_schedule_with_warmup(\r\n",
        "    optimizer, \r\n",
        "    num_warmup_steps=0, \r\n",
        "    num_training_steps=(epochs * len(bert_single_train_loader))\r\n",
        ")\r\n",
        "\r\n",
        "print(\"Model initialised.\")\r\n",
        "\r\n",
        "# Define the loss function\r\n",
        "loss_fn = nn.MSELoss()\r\n",
        "loss_fn = loss_fn.to(device)\r\n",
        "\r\n",
        "results = train(robert_single_train_loader, robert_single_val_loader, model, epochs, loss_fn, optimizer)\r\n",
        "roberta_single_test(model)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aadfb751523748d7a69b19fa5064d1e6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=481.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e59e87581aaa43cea3fae89cfbd0bdd6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=501200538.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Model initialised.\n",
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.3495 | Train RMSE: 0.5912 | Val. Loss: 0.3169 | Val. RMSE: 0.5630 |\n",
            "| Epoch: 02 | Train Loss: 0.3100 | Train RMSE: 0.5568 | Val. Loss: 0.2833 | Val. RMSE: 0.5323 |\n",
            "RMSE = 0.535\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTGSSVzHGVZU"
      },
      "source": [
        "#### RoBERTa Double"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be1giuvD2MyE",
        "outputId": "b9f773ab-8a6e-4cbd-adc8-f1475157f438"
      },
      "source": [
        "## Roberta Both Training\r\n",
        "#=====================================#\r\n",
        "# Define run specific hyperparameters #\r\n",
        "#=====================================#\r\n",
        "lr = 2e-5\r\n",
        "epochs = 4\r\n",
        "#=====================================#\r\n",
        "# Initialise full model\r\n",
        "model = RoBERTa(e_dim, h_dim, batch_size, device).to(device)\r\n",
        "\r\n",
        "# Optimizer and Scheduler setup\r\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\r\n",
        "scheduler = get_linear_schedule_with_warmup(\r\n",
        "    optimizer, \r\n",
        "    num_warmup_steps=0, \r\n",
        "    num_training_steps=(epochs * len(bert_both_train_loader))\r\n",
        ")\r\n",
        "\r\n",
        "print(\"Model initialised.\")\r\n",
        "\r\n",
        "# Define the loss function\r\n",
        "loss_fn = nn.MSELoss()\r\n",
        "loss_fn = loss_fn.to(device)\r\n",
        "\r\n",
        "results = train(robert_both_train_loader, robert_both_val_loader, model, epochs, loss_fn, optimizer)\r\n",
        "roberta_both_test(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model initialised.\n",
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.3466 | Train RMSE: 0.5887 | Val. Loss: 0.3223 | Val. RMSE: 0.5677 |\n",
            "| Epoch: 02 | Train Loss: 0.3210 | Train RMSE: 0.5665 | Val. Loss: 0.3085 | Val. RMSE: 0.5554 |\n",
            "| Epoch: 03 | Train Loss: 0.2792 | Train RMSE: 0.5284 | Val. Loss: 0.3047 | Val. RMSE: 0.5520 |\n",
            "| Epoch: 04 | Train Loss: 0.2359 | Train RMSE: 0.4857 | Val. Loss: 0.3016 | Val. RMSE: 0.5492 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjdNo5yt8NqW"
      },
      "source": [
        "#### Multi-Modal Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkWEEb92mypJ",
        "outputId": "0cdcf8d3-967a-4187-9900-50e9a1931202"
      },
      "source": [
        "## Multi-Model Training\r\n",
        "#=====================================#\r\n",
        "# Define run specific hyperparameters #\r\n",
        "#=====================================#\r\n",
        "lr = 1e-5\r\n",
        "epochs = 20\r\n",
        "#=====================================#\r\n",
        "\r\n",
        "# Create a simple sentence loader\r\n",
        "simple_dataset = Task1Dataset(x_new, x_grades)\r\n",
        "num_train = round(len(simple_dataset) * train_proportion)\r\n",
        "num_val = len(simple_dataset) - num_train\r\n",
        "train_dataset, val_dataset = random_split(simple_dataset, (num_train, num_val))\r\n",
        "\r\n",
        "# Create dataloaders from the edited sentence\r\n",
        "simple_train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, collate_fn=collate_fn_padd_labels)\r\n",
        "simple_val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn_padd_labels)\r\n",
        "\r\n",
        "# Setup the multiple models\r\n",
        "output_dims = 0\r\n",
        "model_r = RoBERTa(e_dim, h_dim, batch_size, device, output_hidden=True).to(device)\r\n",
        "model_r.init_preprocess(RobertaTokenizer.from_pretrained(rob_type, do_lower_case=True))\r\n",
        "output_dims += model_r.output_len\r\n",
        "\r\n",
        "model_b = BERT(e_dim, h_dim, batch_size, device, output_hidden=True).to(device)\r\n",
        "model_b.init_preprocess(BertTokenizer.from_pretrained(bert_type, do_lower_case=True))\r\n",
        "output_dims += model_b.output_len\r\n",
        "\r\n",
        "mod_dict = {'roberta': model_r, 'bert': model_b}\r\n",
        "preprocess = ['roberta', 'bert']\r\n",
        "\r\n",
        "model = MultiModel(mod_dict, preprocess, batch_size, output_dims).to(device)\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\r\n",
        "\r\n",
        "# Define the loss function\r\n",
        "loss_fn = nn.MSELoss()\r\n",
        "loss_fn = loss_fn.to(device)\r\n",
        "\r\n",
        "print(\"Model initialised.\")\r\n",
        "results = train(simple_train_loader, simple_val_loader, model, epochs, loss_fn, optimizer)\r\n",
        "multi_model_testing(model)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model initialised.\n",
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 1.2181 | Train RMSE: 1.1037 | Val. Loss: 1.1243 | Val. RMSE: 1.0604 |\n",
            "| Epoch: 02 | Train Loss: 1.0890 | Train RMSE: 1.0436 | Val. Loss: 1.0037 | Val. RMSE: 1.0018 |\n",
            "| Epoch: 03 | Train Loss: 0.9744 | Train RMSE: 0.9871 | Val. Loss: 0.8970 | Val. RMSE: 0.9471 |\n",
            "| Epoch: 04 | Train Loss: 0.8734 | Train RMSE: 0.9346 | Val. Loss: 0.8031 | Val. RMSE: 0.8961 |\n",
            "| Epoch: 05 | Train Loss: 0.7847 | Train RMSE: 0.8858 | Val. Loss: 0.7211 | Val. RMSE: 0.8492 |\n",
            "| Epoch: 06 | Train Loss: 0.7075 | Train RMSE: 0.8411 | Val. Loss: 0.6502 | Val. RMSE: 0.8063 |\n",
            "| Epoch: 07 | Train Loss: 0.6405 | Train RMSE: 0.8003 | Val. Loss: 0.5890 | Val. RMSE: 0.7674 |\n",
            "| Epoch: 08 | Train Loss: 0.5829 | Train RMSE: 0.7635 | Val. Loss: 0.5366 | Val. RMSE: 0.7325 |\n",
            "| Epoch: 09 | Train Loss: 0.5339 | Train RMSE: 0.7307 | Val. Loss: 0.4922 | Val. RMSE: 0.7016 |\n",
            "| Epoch: 10 | Train Loss: 0.4929 | Train RMSE: 0.7020 | Val. Loss: 0.4556 | Val. RMSE: 0.6750 |\n",
            "| Epoch: 11 | Train Loss: 0.4585 | Train RMSE: 0.6771 | Val. Loss: 0.4250 | Val. RMSE: 0.6519 |\n",
            "| Epoch: 12 | Train Loss: 0.4305 | Train RMSE: 0.6561 | Val. Loss: 0.4008 | Val. RMSE: 0.6331 |\n",
            "| Epoch: 13 | Train Loss: 0.4081 | Train RMSE: 0.6388 | Val. Loss: 0.3815 | Val. RMSE: 0.6176 |\n",
            "| Epoch: 14 | Train Loss: 0.3905 | Train RMSE: 0.6249 | Val. Loss: 0.3667 | Val. RMSE: 0.6055 |\n",
            "| Epoch: 15 | Train Loss: 0.3770 | Train RMSE: 0.6140 | Val. Loss: 0.3554 | Val. RMSE: 0.5962 |\n",
            "| Epoch: 16 | Train Loss: 0.3670 | Train RMSE: 0.6058 | Val. Loss: 0.3475 | Val. RMSE: 0.5895 |\n",
            "| Epoch: 17 | Train Loss: 0.3598 | Train RMSE: 0.5998 | Val. Loss: 0.3418 | Val. RMSE: 0.5846 |\n",
            "| Epoch: 18 | Train Loss: 0.3547 | Train RMSE: 0.5956 | Val. Loss: 0.3380 | Val. RMSE: 0.5814 |\n",
            "| Epoch: 19 | Train Loss: 0.3513 | Train RMSE: 0.5927 | Val. Loss: 0.3357 | Val. RMSE: 0.5794 |\n",
            "| Epoch: 20 | Train Loss: 0.3492 | Train RMSE: 0.5909 | Val. Loss: 0.3342 | Val. RMSE: 0.5781 |\n",
            "RMSE = 0.580\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJYjKQMPOq6W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35bf9c7f-268f-4abd-aaed-e6ef655e31a6"
      },
      "source": [
        "## Multi-Model Training where we train each model individually first then train the final concatenation layer\r\n",
        "#=====================================#\r\n",
        "# Define run specific hyperparameters #\r\n",
        "#=====================================#\r\n",
        "lr_r = 2e-5\r\n",
        "epochs_r = 2\r\n",
        "lr_b = 15e-6\r\n",
        "epochs_b = 2\r\n",
        "#=====================================#\r\n",
        "\r\n",
        "# Create a simple sentence loader\r\n",
        "simple_dataset = Task1Dataset(x_new, x_grades)\r\n",
        "num_train = round(len(simple_dataset) * train_proportion)\r\n",
        "num_val = len(simple_dataset) - num_train\r\n",
        "train_dataset, val_dataset = random_split(simple_dataset, (num_train, num_val))\r\n",
        "\r\n",
        "# Create dataloaders from the edited sentence\r\n",
        "simple_train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, collate_fn=collate_fn_padd_labels)\r\n",
        "simple_val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn_padd_labels)\r\n",
        "\r\n",
        "# Setup the individual models\r\n",
        "output_dims = 0\r\n",
        "model_r = RoBERTa(e_dim, h_dim, batch_size, device).to(device)\r\n",
        "model_r.init_preprocess(RobertaTokenizer.from_pretrained(rob_type, do_lower_case=True))\r\n",
        "output_dims += model_r.output_len\r\n",
        "\r\n",
        "model_b = BERT(e_dim, h_dim, batch_size, device).to(device)\r\n",
        "model_b.init_preprocess(BertTokenizer.from_pretrained(bert_type, do_lower_case=True))\r\n",
        "output_dims += model_b.output_len\r\n",
        "\r\n",
        "loss_fn_r = nn.MSELoss()\r\n",
        "loss_fn_r = loss_fn_r.to(device)\r\n",
        "\r\n",
        "loss_fn_b = nn.MSELoss()\r\n",
        "loss_fn_b = loss_fn_b.to(device)\r\n",
        "\r\n",
        "optimizer_r = torch.optim.Adam(model_r.parameters(), lr=lr_r)\r\n",
        "optimizer_b = torch.optim.Adam(model_b.parameters(), lr=lr_b)\r\n",
        "\r\n",
        "# Train the individual models\r\n",
        "print(\"RoBERTa\")\r\n",
        "results_r = train(robert_single_train_loader, robert_single_train_loader, model_r, epochs_r, loss_fn_r, optimizer_r)\r\n",
        "print(\"BERT\")\r\n",
        "results_b = train(bert_single_train_loader, bert_single_val_loader, model_b, epochs_b, loss_fn_b, optimizer_b)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RoBERTa\n",
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.3532 | Train RMSE: 0.5943 | Val. Loss: 0.3187 | Val. RMSE: 0.5646 |\n",
            "| Epoch: 02 | Train Loss: 0.3122 | Train RMSE: 0.5587 | Val. Loss: 0.2655 | Val. RMSE: 0.5153 |\n",
            "BERT\n",
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.3395 | Train RMSE: 0.5826 | Val. Loss: 0.3039 | Val. RMSE: 0.5513 |\n",
            "| Epoch: 02 | Train Loss: 0.2567 | Train RMSE: 0.5066 | Val. Loss: 0.2856 | Val. RMSE: 0.5344 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5N7KbUfzcA8U",
        "outputId": "22f33d1e-430b-48a2-bea6-710c0c96bd82"
      },
      "source": [
        "#=====================================#\r\n",
        "# Define run specific hyperparameters #\r\n",
        "#=====================================#\r\n",
        "lr = 1e-3\r\n",
        "epochs = 5\r\n",
        "#=====================================#\r\n",
        "# Model Freeze\r\n",
        "model_r.train()\r\n",
        "model_r.freeze_embeddings()\r\n",
        "model_b.train()\r\n",
        "model_b.freeze_embeddings()\r\n",
        "\r\n",
        "mod_dict = {'roberta': model_r, 'bert': model_b}\r\n",
        "preprocess = ['roberta', 'bert']\r\n",
        "\r\n",
        "model = MultiModel(mod_dict, preprocess, batch_size, output_dims).to(device)\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\r\n",
        "\r\n",
        "# Define the loss function\r\n",
        "loss_fn = nn.MSELoss()\r\n",
        "loss_fn = loss_fn.to(device)\r\n",
        "\r\n",
        "print(\"Model initialised.\")\r\n",
        "results = train(simple_train_loader, simple_val_loader, model, epochs, loss_fn, optimizer)\r\n",
        "multi_model_testing(model)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model initialised.\n",
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 1.2374 | Train RMSE: 1.1124 | Val. Loss: 0.6971 | Val. RMSE: 0.8349 |\n",
            "| Epoch: 02 | Train Loss: 0.4462 | Train RMSE: 0.6680 | Val. Loss: 0.2929 | Val. RMSE: 0.5412 |\n",
            "| Epoch: 03 | Train Loss: 0.2373 | Train RMSE: 0.4871 | Val. Loss: 0.2083 | Val. RMSE: 0.4564 |\n",
            "| Epoch: 04 | Train Loss: 0.2042 | Train RMSE: 0.4519 | Val. Loss: 0.1998 | Val. RMSE: 0.4470 |\n",
            "| Epoch: 05 | Train Loss: 0.2008 | Train RMSE: 0.4481 | Val. Loss: 0.1966 | Val. RMSE: 0.4434 |\n",
            "RMSE = 0.536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsFTU7vrT7B1"
      },
      "source": [
        "#### BiLSTM Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRRz_Nhbs1kB"
      },
      "source": [
        "Simple BiLSTM on Glove Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JKbLuqRswjG"
      },
      "source": [
        "## Using the Glove corpus on a simple BiLSTM layer\r\n",
        "#=====================================#\r\n",
        "# Define run specific hyperparameters #\r\n",
        "#=====================================#\r\n",
        "lr = 0.001\r\n",
        "epochs = 6\r\n",
        "#=====================================#\r\n",
        "\r\n",
        "model = BiLSTM(glove_e_dim, h_dim, glove_input_dim, batch_size, device).to(device)\r\n",
        "# We provide the model with our embeddings\r\n",
        "model.embedding.weight.data.copy_(torch.from_numpy(glove_wvecs))\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\r\n",
        "\r\n",
        "# Define the loss function\r\n",
        "loss_fn = nn.MSELoss()\r\n",
        "loss_fn = loss_fn.to(device)\r\n",
        "\r\n",
        "print(\"Model initialised.\")\r\n",
        "results = train(glove_train_loader, glove_test_loader, model, epochs, loss_fn, optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPUNRkCrF9UH"
      },
      "source": [
        "#### GLoVe Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r7fBv9TGORJ"
      },
      "source": [
        "GLoVe-specific Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szwSEigDF9UW"
      },
      "source": [
        "## Using the Glove corpus on a simple BiLSTM layer\r\n",
        "#=====================================#\r\n",
        "# Define run specific hyperparameters #\r\n",
        "#=====================================### Approach 1 code, using functions defined above:\r\n",
        "\r\n",
        "# We set our training data and test data\r\n",
        "training_data = train_df['original']\r\n",
        "test_data = test_df['original']\r\n",
        "true_test_data = true_test_df['original']\r\n",
        "\r\n",
        "# Creating word vectors\r\n",
        "training_vocab, training_tokenized_corpus = create_vocab(training_data)\r\n",
        "test_vocab, test_tokenized_corpus = create_vocab(test_data)\r\n",
        "true_test_vocab, true_test_tokenized_corpus = create_vocab(true_test_data)\r\n",
        "\r\n",
        "# Creating joint vocab from test and train:\r\n",
        "joint_vocab, joint_tokenized_corpus = create_vocab(pd.concat([training_data, test_data, true_test_data]))\r\n",
        "\r\n",
        "print(\"Vocab created.\")\r\n",
        "\r\n",
        "# We create representations for our tokens\r\n",
        "wvecs = [] # word vectors\r\n",
        "word2idx = [] # word2index\r\n",
        "idx2word = []\r\n",
        "\r\n",
        "# This is a large file, it will take a while to load in the memory!\r\n",
        "with codecs.open('glove.6B.100d.txt', 'r','utf-8') as f:\r\n",
        "  index = 1\r\n",
        "  for line in f.readlines():\r\n",
        "    # Ignore the first line - first line typically contains vocab, dimensionality\r\n",
        "    if len(line.strip().split()) > 3:\r\n",
        "      word = line.strip().split()[0]\r\n",
        "      if word in joint_vocab:\r\n",
        "          (word, vec) = (word,\r\n",
        "                     list(map(float,line.strip().split()[1:])))\r\n",
        "          wvecs.append(vec)\r\n",
        "          word2idx.append((word, index))\r\n",
        "          idx2word.append((index, word))\r\n",
        "          index += 1\r\n",
        "\r\n",
        "wvecs = np.array(wvecs)\r\n",
        "word2idx = dict(word2idx)\r\n",
        "idx2word = dict(idx2word)\r\n",
        "\r\n",
        "vectorized_seqs = [[word2idx[tok] for tok in seq if tok in word2idx] for seq in training_tokenized_corpus]\r\n",
        "\r\n",
        "# To avoid any sentences being empty (if no words match to our word embeddings)\r\n",
        "vectorized_seqs = [x if len(x) > 0 else [0] for x in vectorized_seqs]\r\n",
        "\r\n",
        "INPUT_DIM = len(word2idx)\r\n",
        "EMBEDDING_DIM = 100\r\n",
        "BATCH_SIZE = 32\r\n",
        "\r\n",
        "model = BiLSTM(EMBEDDING_DIM, 50, INPUT_DIM, BATCH_SIZE, device)\r\n",
        "print(\"Model initialised.\")\r\n",
        "\r\n",
        "model.to(device)\r\n",
        "# We provide the model with our embeddings\r\n",
        "model.embedding.weight.data.copy_(torch.from_numpy(wvecs))\r\n",
        "\r\n",
        "feature = vectorized_seqs\r\n",
        "\r\n",
        "# 'feature' is a list of lists, each containing embedding IDs for word tokens\r\n",
        "train_and_dev = Task1Dataset(feature, train_df['meanGrade'])\r\n",
        "\r\n",
        "train_examples = round(len(train_and_dev)*train_proportion)\r\n",
        "dev_examples = len(train_and_dev) - train_examples\r\n",
        "\r\n",
        "train_dataset, dev_dataset = random_split(train_and_dev,\r\n",
        "                                           (train_examples,\r\n",
        "                                            dev_examples))\r\n",
        "\r\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\r\n",
        "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\r\n",
        "\r\n",
        "print(\"Dataloaders created.\")\r\n",
        "\r\n",
        "loss_fn = nn.MSELoss()\r\n",
        "loss_fn = loss_fn.to(device)\r\n",
        "\r\n",
        "optimizer = torch.optim.Adam(model.parameters())\r\n",
        "\r\n",
        "train(train_loader, dev_loader, model, epochs)\r\n",
        "lr = 0.001\r\n",
        "epochs = 6\r\n",
        "#=====================================#\r\n",
        "\r\n",
        "model = BiLSTM(glove_e_dim, h_dim, glove_input_dim, batch_size, device).to(device)\r\n",
        "# We provide the model with our embeddings\r\n",
        "model.embedding.weight.data.copy_(torch.from_numpy(glove_wvecs))\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\r\n",
        "\r\n",
        "# Define the loss function\r\n",
        "loss_fn = nn.MSELoss()\r\n",
        "loss_fn = loss_fn.to(device)\r\n",
        "\r\n",
        "print(\"Model initialised.\")\r\n",
        "results = train(glove_train_loader, glove_test_loader, model, epochs, loss_fn, optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6R7Tkm1GUDR"
      },
      "source": [
        "GloVe Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuutgufHGWIU"
      },
      "source": [
        "vectorized_seqs = [[word2idx[tok] for tok in seq if tok in word2idx] for seq in true_test_tokenized_corpus]\r\n",
        "\r\n",
        "# To avoid any sentences being empty (if no words match to our word embeddings)\r\n",
        "feature = [x if len(x) > 0 else [0] for x in vectorized_seqs]\r\n",
        "\r\n",
        "test_ids = true_test_df['id']\r\n",
        "test_dataset = Task1Dataset(feature, test_ids)\r\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\r\n",
        "\r\n",
        "predictions, pred_ids = get_predictions(test_dataloader, model)\r\n",
        "pd.DataFrame({'id': pred_ids, 'pred': predictions}).to_csv(\"./predictions.csv\")\r\n",
        "score_task_1('./test.csv', './predictions.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0lEX6XGT29m"
      },
      "source": [
        "#### Cross Validation (Not working anymore after refactor, but we stuck with the best parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5J5HmT_T6tY"
      },
      "source": [
        "grid = {\r\n",
        "    'learning_rate': [1e-3, 1e-4, 1e-5, 1e-6],\r\n",
        "    'batch_size': [32, 64]\r\n",
        "}\r\n",
        "\r\n",
        "# scorer = make_scorer(make_custom_score, needs_proba=True)\r\n",
        "paramGrid = ParameterGrid(grid)\r\n",
        "cv = KFold(5)\r\n",
        "labels = train_df['meanGrade']\r\n",
        "\r\n",
        "train_and_dev = Task1Dataset(vectorized_seqs, train_df['meanGrade'])\r\n",
        "\r\n",
        "# Cross Validation\r\n",
        "\r\n",
        "for params in paramGrid:\r\n",
        "\r\n",
        "    print(\"Current parameters: \" + str(params))\r\n",
        "    fold = 0;\r\n",
        "\r\n",
        "    optimizer = torch.optim.Adam(model.parameters(), params[\"learning_rate\"])\r\n",
        "    for train_indices, dev_indices in cv.split(vectorized_seqs, labels):\r\n",
        "\r\n",
        "        fold += 1;\r\n",
        "        print(\"Current fold: \" + str(fold))\r\n",
        "\r\n",
        "        train_dataset = torch.utils.data.Subset(train_and_dev, train_indices)\r\n",
        "        dev_dataset = torch.utils.data.Subset(train_and_dev, dev_indices)\r\n",
        "\r\n",
        "        print(\"Dataloaders created.\")\r\n",
        "\r\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\r\n",
        "        dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\r\n",
        "\r\n",
        "        train(train_loader, dev_loader, model, epochs, loss_fn, optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMA3mumakDSZ"
      },
      "source": [
        "### Models (Approach 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db5Li_oAsskV"
      },
      "source": [
        "Simple BiLSTM on Brown Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "0_4Benjk3BQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4678667-ee7e-43bf-cf0e-2f4534f84c8d"
      },
      "source": [
        "## Using the our Brown corpus on a simple BiLSTM layer\n",
        "#=====================================#\n",
        "# Define run specific hyperparameters #\n",
        "#=====================================#\n",
        "lr = 0.001\n",
        "epochs = 6\n",
        "#=====================================#\n",
        "\n",
        "model = BiLSTM(brown_e_dim, h_dim, brown_input_dim, batch_size, device).to(device)\n",
        "# We provide the model with our embeddings\n",
        "model.embedding.weight.data.copy_(torch.from_numpy(brown_wvecs))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.MSELoss()\n",
        "loss_fn = loss_fn.to(device)\n",
        "\n",
        "print(\"Model initialised.\")\n",
        "results = train(brown_train_loader, brown_test_loader, model, epochs, loss_fn, optimizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model initialised.\n",
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.3650 | Train RMSE: 0.6042 | Val. Loss: 0.3453 | Val. RMSE: 0.5876 |\n",
            "| Epoch: 02 | Train Loss: 0.3379 | Train RMSE: 0.5813 | Val. Loss: 0.3441 | Val. RMSE: 0.5866 |\n",
            "| Epoch: 03 | Train Loss: 0.3156 | Train RMSE: 0.5618 | Val. Loss: 0.3406 | Val. RMSE: 0.5836 |\n",
            "| Epoch: 04 | Train Loss: 0.2708 | Train RMSE: 0.5204 | Val. Loss: 0.3620 | Val. RMSE: 0.6017 |\n",
            "| Epoch: 05 | Train Loss: 0.2436 | Train RMSE: 0.4936 | Val. Loss: 0.3697 | Val. RMSE: 0.6080 |\n",
            "| Epoch: 06 | Train Loss: 0.2251 | Train RMSE: 0.4745 | Val. Loss: 0.3731 | Val. RMSE: 0.6108 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYYfOat7AJ8E"
      },
      "source": [
        "Simple regression model on extracted features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewkWUrP33BQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29956687-15c0-41fc-c1bc-b68f9559b660"
      },
      "source": [
        "# Using hand-crafted features on a LinearRegressor model.\n",
        "extracted_df = add_extracted_features(train_df)\n",
        "numeric_data = extracted_df.select_dtypes(['number']).drop(columns='id')\n",
        "\n",
        "x = numeric_data.drop(columns=['meanGrade', 'grades']).values\n",
        "y = numeric_data['meanGrade'].values\n",
        "\n",
        "training_data, dev_data, training_y, dev_y = train_test_split(x, y, test_size=(1-train_proportion),\n",
        "                                                                        random_state=42)\n",
        "\n",
        "regression_model = LinearRegression().fit(training_data, training_y)\n",
        "\n",
        "# Train predictions\n",
        "predicted_train = regression_model.predict(training_data)\n",
        "\n",
        "# Dev predictions\n",
        "predicted = regression_model.predict(dev_data)\n",
        "\n",
        "# We run the evaluation:\n",
        "print(\"\\nTrain performance:\")\n",
        "sse, mse = model_performance(predicted_train, training_y, True)\n",
        "\n",
        "print(\"\\nDev performance:\")\n",
        "sse, mse = model_performance(predicted, dev_y, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train performance:\n",
            "| MSE: 0.34 | RMSE: 0.58 |\n",
            "\n",
            "Dev performance:\n",
            "| MSE: 0.34 | RMSE: 0.58 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjVcd4pYAQH7"
      },
      "source": [
        "Simple FNN on extracted features + TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O0Ajan7ASSM"
      },
      "source": [
        "# Define simple model architecture.\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Net, self).__init__()                    \n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)                \n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, x):                              \n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KGbTrzK9Bae5",
        "outputId": "9081a2f9-25ec-4e19-8887-4bcebcd6302b"
      },
      "source": [
        "extracted_df = add_extracted_features(train_df)\n",
        "preprocessed_df = get_processed_df(train_df)\n",
        "\n",
        "numeric_data = extracted_df.select_dtypes(['number']).drop(columns='id')\n",
        "tfidf_sentence = get_tfidf(preprocessed_df['editedSentence'])\n",
        "\n",
        "x = numeric_data.drop(columns=['meanGrade', 'grades']).values\n",
        "x = np.concatenate((tfidf_sentence.toarray(), x), axis=1)\n",
        "y = numeric_data['meanGrade'].values\n",
        "\n",
        "# Prepare data.\n",
        "training_data, dev_data, training_y, dev_y = train_test_split(x, y, test_size=(1-train_proportion),\n",
        "                                                                        random_state=42)\n",
        "x_train = torch.Tensor(training_data).cuda()\n",
        "y_train = torch.Tensor(training_y).cuda()\n",
        "x_test = torch.Tensor(dev_data).cuda()\n",
        "y_test = torch.Tensor(dev_y).cuda()\n",
        "\n",
        "# Initialise model.\n",
        "net = Net(input_size=x.shape[1], hidden_size=20, output_size=1)\n",
        "net.cuda()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.2)\n",
        "loss_func = torch.nn.MSELoss()\n",
        "\n",
        "num_epochs = 40\n",
        "\n",
        "train_loss = []\n",
        "test_loss = []\n",
        "\n",
        "# Train the network\n",
        "for t in range(num_epochs):\n",
        "    tr_loss = 0\n",
        "    te_loss = 0\n",
        "    prediction = net(x_train)     \n",
        "\n",
        "    tr_loss = loss_func(prediction, y_train)\n",
        "    train_loss.append(tr_loss/len(x_train))\n",
        "\n",
        "    optimizer.zero_grad()   \n",
        "    tr_loss.backward()        \n",
        "    optimizer.step()       \n",
        "    \n",
        "    with torch.no_grad():\n",
        "      test_pred = net(x_test)\n",
        "      te_loss = loss_func(test_pred, y_test)\n",
        "      test_loss.append(te_loss/len(x_test))\n",
        "   \n",
        "    print(\"Epoch: {} | Train loss: {} | Test loss: {}\".format(t+1, tr_loss/len(x_train), te_loss/len(x_test)))\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,7))\n",
        "ax.set_title('Loss Plots')\n",
        "ax.set_xlabel('Epochs')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_yscale('log')\n",
        "ax.plot(list(range(t+1)), train_loss, label=\"Train Loss\")\n",
        "ax.plot(list(range(t+1)), test_loss, label=\"Test Loss\")\n",
        "fig.legend()\n",
        "\n",
        "# Test\n",
        "predicted_train = net(x_train).cpu().detach().numpy()\n",
        "predicted_test = net(x_test).cpu().detach().numpy()\n",
        "print(\"\\nTrain performance:\")\n",
        "sse, mse = model_performance(predicted_train, training_y, True)\n",
        "print(\"\\nDev performance:\")\n",
        "sse, mse = model_performance(predicted_test, dev_y, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([7721])) that is different to the input size (torch.Size([7721, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([1931])) that is different to the input size (torch.Size([1931, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 | Train loss: 0.0002180482551921159 | Test loss: 18.506433486938477\n",
            "Epoch: 2 | Train loss: 4.55071496963501 | Test loss: 2.2326202392578125\n",
            "Epoch: 3 | Train loss: 0.5582336187362671 | Test loss: 0.8039882779121399\n",
            "Epoch: 4 | Train loss: 0.2009923756122589 | Test loss: 0.28962793946266174\n",
            "Epoch: 5 | Train loss: 0.0723855122923851 | Test loss: 0.10442624986171722\n",
            "Epoch: 6 | Train loss: 0.026087049394845963 | Test loss: 0.0377345085144043\n",
            "Epoch: 7 | Train loss: 0.009419599547982216 | Test loss: 0.013714012689888477\n",
            "Epoch: 8 | Train loss: 0.0034193212632089853 | Test loss: 0.0050597465597093105\n",
            "Epoch: 9 | Train loss: 0.001259221346117556 | Test loss: 0.0019400769378989935\n",
            "Epoch: 10 | Train loss: 0.00048158527351915836 | Test loss: 0.0008145159226842225\n",
            "Epoch: 11 | Train loss: 0.00020163645967841148 | Test loss: 0.00040782600990496576\n",
            "Epoch: 12 | Train loss: 0.00010085484245792031 | Test loss: 0.0002605248591862619\n",
            "Epoch: 13 | Train loss: 6.457346898969263e-05 | Test loss: 0.0002069607871817425\n",
            "Epoch: 14 | Train loss: 5.1512168283807114e-05 | Test loss: 0.00018735628691501915\n",
            "Epoch: 15 | Train loss: 4.681009886553511e-05 | Test loss: 0.00018010583880823106\n",
            "Epoch: 16 | Train loss: 4.511735824053176e-05 | Test loss: 0.0001773799885995686\n",
            "Epoch: 17 | Train loss: 4.450796768651344e-05 | Test loss: 0.00017632925300858915\n",
            "Epoch: 18 | Train loss: 4.4288575736572966e-05 | Test loss: 0.0001759093429427594\n",
            "Epoch: 19 | Train loss: 4.420960249262862e-05 | Test loss: 0.00017573317745700479\n",
            "Epoch: 20 | Train loss: 4.4181186240166426e-05 | Test loss: 0.00017565475718583912\n",
            "Epoch: 21 | Train loss: 4.417094532982446e-05 | Test loss: 0.0001756175042828545\n",
            "Epoch: 22 | Train loss: 4.416725641931407e-05 | Test loss: 0.00017559871776029468\n",
            "Epoch: 23 | Train loss: 4.41659358330071e-05 | Test loss: 0.00017558870604261756\n",
            "Epoch: 24 | Train loss: 4.4165451981825754e-05 | Test loss: 0.00017558316176291555\n",
            "Epoch: 25 | Train loss: 4.4165262806927785e-05 | Test loss: 0.000175580003997311\n",
            "Epoch: 26 | Train loss: 4.416522278916091e-05 | Test loss: 0.00017557817045599222\n",
            "Epoch: 27 | Train loss: 4.416519368533045e-05 | Test loss: 0.00017557709361426532\n",
            "Epoch: 28 | Train loss: 4.416520459926687e-05 | Test loss: 0.00017557643877808005\n",
            "Epoch: 29 | Train loss: 4.4165186409372836e-05 | Test loss: 0.00017557607498019934\n",
            "Epoch: 30 | Train loss: 4.416518277139403e-05 | Test loss: 0.00017557584214955568\n",
            "Epoch: 31 | Train loss: 4.4165186409372836e-05 | Test loss: 0.0001755756966304034\n",
            "Epoch: 32 | Train loss: 4.416517913341522e-05 | Test loss: 0.00017557562387082726\n",
            "Epoch: 33 | Train loss: 4.4165190047351643e-05 | Test loss: 0.00017557556566316634\n",
            "Epoch: 34 | Train loss: 4.4165186409372836e-05 | Test loss: 0.0001755755365593359\n",
            "Epoch: 35 | Train loss: 4.416517185745761e-05 | Test loss: 0.00017557550745550543\n",
            "Epoch: 36 | Train loss: 4.416518277139403e-05 | Test loss: 0.0001755755365593359\n",
            "Epoch: 37 | Train loss: 4.4165186409372836e-05 | Test loss: 0.00017557552200742066\n",
            "Epoch: 38 | Train loss: 4.416519368533045e-05 | Test loss: 0.00017557550745550543\n",
            "Epoch: 39 | Train loss: 4.4165186409372836e-05 | Test loss: 0.00017557550745550543\n",
            "Epoch: 40 | Train loss: 4.4165186409372836e-05 | Test loss: 0.00017557550745550543\n",
            "\n",
            "Train performance:\n",
            "| MSE: 0.34 | RMSE: 0.58 |\n",
            "\n",
            "Dev performance:\n",
            "| MSE: 0.34 | RMSE: 0.58 |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb84ziTQFzvY"
      },
      "source": [
        "FNN on additional FunLines dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvZ242XMF7-l",
        "outputId": "f71f8255-42a5-4253-b7f6-9fed06544c17"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXXzQCLRGMYp"
      },
      "source": [
        "# download data\n",
        "funlines_train_df = pd.read_csv(\"/content/gdrive/MyDrive/YEAR4/NLP/data/train_funlines.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nKXuusmGecG"
      },
      "source": [
        "# Prepare FunLines dataset.\n",
        "x_og, x_new, x_og_word, x_new_word = get_sentences(funlines_train_df)\n",
        "y_og, y_new, y_og_word, y_new_word = get_sentences(funlines_train_df)\n",
        "x_grades = funlines_train_df['meanGrade']\n",
        "\n",
        "# Insert these new sentences into the dataframe\n",
        "funlines_train_df['editedSentence'] = x_new\n",
        "funlines_train_df['replaceWord'] = x_og_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GFLheaNQFyFB",
        "outputId": "a9616cd0-17d5-4822-d8ed-9c9014cb3fb2"
      },
      "source": [
        "orig_extracted_df = add_extracted_features(train_df)\n",
        "funlines_extracted_df = add_extracted_features(funlines_train_df)\n",
        "\n",
        "numeric_data = orig_extracted_df.select_dtypes(['number']).drop(columns='id')\n",
        "funlines_numeric_data = funlines_extracted_df.select_dtypes(['number']).drop(columns='id')\n",
        "\n",
        "x = numeric_data.drop(columns=['meanGrade', 'grades']).values\n",
        "y = numeric_data['meanGrade'].values\n",
        "\n",
        "# Prepare data.\n",
        "training_data, dev_data, training_y, dev_y = train_test_split(x, y, test_size=(1-train_proportion),\n",
        "                                                                        random_state=42)\n",
        "\n",
        "# Append additional training data.\n",
        "funlines_x = funlines_numeric_data.drop(columns=['meanGrade', 'grades']).values\n",
        "funlines_y = funlines_numeric_data['meanGrade'].values\n",
        "training_data = np.concatenate((training_data, funlines_x), axis=0)\n",
        "training_y = np.concatenate((training_y, funlines_y), axis=0)\n",
        "\n",
        "x_train = torch.Tensor(training_data).cuda()\n",
        "y_train = torch.Tensor(training_y).cuda()\n",
        "x_test = torch.Tensor(dev_data).cuda()\n",
        "y_test = torch.Tensor(dev_y).cuda()\n",
        "\n",
        "# Initialise model.\n",
        "net = Net(input_size=x.shape[1], hidden_size=20, output_size=1)\n",
        "net.cuda()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.2)\n",
        "loss_func = torch.nn.MSELoss()\n",
        "\n",
        "num_epochs = 40\n",
        "\n",
        "train_loss = []\n",
        "test_loss = []\n",
        "\n",
        "# Train the network\n",
        "for t in range(num_epochs):\n",
        "    tr_loss = 0\n",
        "    te_loss = 0\n",
        "    prediction = net(x_train)     \n",
        "\n",
        "    tr_loss = loss_func(prediction, y_train)\n",
        "    train_loss.append(tr_loss/len(x_train))\n",
        "\n",
        "    optimizer.zero_grad()   \n",
        "    tr_loss.backward()        \n",
        "    optimizer.step()       \n",
        "    \n",
        "    with torch.no_grad():\n",
        "      test_pred = net(x_test)\n",
        "      te_loss = loss_func(test_pred, y_test)\n",
        "      test_loss.append(te_loss/len(x_test))\n",
        "   \n",
        "    print(\"Epoch: {} | Train loss: {} | Test loss: {}\".format(t+1, tr_loss/len(x_train), te_loss/len(x_test)))\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,7))\n",
        "ax.set_title('Loss Plots')\n",
        "ax.set_xlabel('Epochs')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_yscale('log')\n",
        "ax.plot(list(range(t+1)), train_loss, label=\"Train Loss\")\n",
        "ax.plot(list(range(t+1)), test_loss, label=\"Test Loss\")\n",
        "fig.legend()\n",
        "\n",
        "# Test\n",
        "predicted_train = net(x_train).cpu().detach().numpy()\n",
        "predicted_test = net(x_test).cpu().detach().numpy()\n",
        "print(\"\\nTrain performance:\")\n",
        "sse, mse = model_performance(predicted_train, training_y, True)\n",
        "print(\"\\nDev performance:\")\n",
        "sse, mse = model_performance(predicted_test, dev_y, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([15969])) that is different to the input size (torch.Size([15969, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([1931])) that is different to the input size (torch.Size([1931, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 | Train loss: 0.01633569970726967 | Test loss: 392794976.0\n",
            "Epoch: 2 | Train loss: 47671104.0 | Test loss: 48524284.0\n",
            "Epoch: 3 | Train loss: 5867649.0 | Test loss: 17468726.0\n",
            "Epoch: 4 | Train loss: 2112353.75 | Test loss: 6288733.0\n",
            "Epoch: 5 | Train loss: 760447.1875 | Test loss: 2263939.0\n",
            "Epoch: 6 | Train loss: 273760.90625 | Test loss: 815015.375\n",
            "Epoch: 7 | Train loss: 98553.9140625 | Test loss: 293403.84375\n",
            "Epoch: 8 | Train loss: 35479.4140625 | Test loss: 105624.3984375\n",
            "Epoch: 9 | Train loss: 12772.587890625 | Test loss: 38024.19921875\n",
            "Epoch: 10 | Train loss: 4598.1328125 | Test loss: 13688.3544921875\n",
            "Epoch: 11 | Train loss: 1655.3271484375 | Test loss: 4927.59521484375\n",
            "Epoch: 12 | Train loss: 595.9178466796875 | Test loss: 1773.806640625\n",
            "Epoch: 13 | Train loss: 214.5304412841797 | Test loss: 638.4938354492188\n",
            "Epoch: 14 | Train loss: 77.23097229003906 | Test loss: 229.81192016601562\n",
            "Epoch: 15 | Train loss: 27.803157806396484 | Test loss: 82.70486450195312\n",
            "Epoch: 16 | Train loss: 10.00915241241455 | Test loss: 29.75732421875\n",
            "Epoch: 17 | Train loss: 3.603309154510498 | Test loss: 10.702832221984863\n",
            "Epoch: 18 | Train loss: 1.2972054481506348 | Test loss: 3.8471851348876953\n",
            "Epoch: 19 | Train loss: 0.46700814366340637 | Test loss: 1.3815348148345947\n",
            "Epoch: 20 | Train loss: 0.16813717782497406 | Test loss: 0.49533000588417053\n",
            "Epoch: 21 | Train loss: 0.060543570667505264 | Test loss: 0.17715388536453247\n",
            "Epoch: 22 | Train loss: 0.02180989272892475 | Test loss: 0.06312503665685654\n",
            "Epoch: 23 | Train loss: 0.007865755818784237 | Test loss: 0.02238341048359871\n",
            "Epoch: 24 | Train loss: 0.0028458693996071815 | Test loss: 0.007901681587100029\n",
            "Epoch: 25 | Train loss: 0.0010387097718194127 | Test loss: 0.002799410605803132\n",
            "Epoch: 26 | Train loss: 0.000388132844818756 | Test loss: 0.0010292837396264076\n",
            "Epoch: 27 | Train loss: 0.0001539249933557585 | Test loss: 0.00043205206748098135\n",
            "Epoch: 28 | Train loss: 6.961013423278928e-05 | Test loss: 0.00024105746706482023\n",
            "Epoch: 29 | Train loss: 3.9256792661035433e-05 | Test loss: 0.0001867045502876863\n",
            "Epoch: 30 | Train loss: 2.8329586712061428e-05 | Test loss: 0.0001757806312525645\n",
            "Epoch: 31 | Train loss: 2.4395792934228666e-05 | Test loss: 0.00017703385674394667\n",
            "Epoch: 32 | Train loss: 2.2979622372076847e-05 | Test loss: 0.00018059655849356204\n",
            "Epoch: 33 | Train loss: 2.2469805116998032e-05 | Test loss: 0.00018374602950643748\n",
            "Epoch: 34 | Train loss: 2.228626726719085e-05 | Test loss: 0.00018600000475998968\n",
            "Epoch: 35 | Train loss: 2.2220205210032873e-05 | Test loss: 0.00018748351430986077\n",
            "Epoch: 36 | Train loss: 2.219641100964509e-05 | Test loss: 0.00018842084682546556\n",
            "Epoch: 37 | Train loss: 2.218785448349081e-05 | Test loss: 0.00018900021677836776\n",
            "Epoch: 38 | Train loss: 2.2184769477462396e-05 | Test loss: 0.00018935397383756936\n",
            "Epoch: 39 | Train loss: 2.2183661712915637e-05 | Test loss: 0.00018956843996420503\n",
            "Epoch: 40 | Train loss: 2.2183259716257453e-05 | Test loss: 0.0001896979083539918\n",
            "\n",
            "Train performance:\n",
            "| MSE: 0.35 | RMSE: 0.60 |\n",
            "\n",
            "Dev performance:\n",
            "| MSE: 0.37 | RMSE: 0.61 |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA5bYyuG3BQj"
      },
      "source": [
        "### Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72fIC0-I3BQk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "outputId": "73d9925d-d3c7-424a-dd33-bcb5e0cb7ad0"
      },
      "source": [
        "# Baseline for the task\n",
        "pred_baseline = torch.zeros(len(dev_y)) + np.mean(training_y)\n",
        "print(\"\\nBaseline performance:\")\n",
        "sse, mse = model_performance(pred_baseline, dev_y, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Baseline performance:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-3c7b5eafe872>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpred_baseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nBaseline performance:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_baseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-54-94966066193b>\u001b[0m in \u001b[0;36mmodel_performance\u001b[0;34m(output, target, print_output)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \"\"\"\n\u001b[1;32m      6\u001b[0m   \u001b[0msq_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0msse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msq_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msq_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2241\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2242\u001b[0;31m                           initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: sum() received an invalid combination of arguments - got (out=NoneType, axis=NoneType, ), but expected one of:\n * (*, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: out, axis\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtEePKZ5Nb-S"
      },
      "source": [
        "# Analysis\r\n",
        "Here we perform some analysis on the data distribution to see if there is any significant features that can be handcrafted and added to the training of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QoJp85EOYtw"
      },
      "source": [
        "## Sentence Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqdY513cOgtl"
      },
      "source": [
        "### Grade distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY14ISU1NozJ"
      },
      "source": [
        "grades = train_df['meanGrade']\r\n",
        "print(\"Mean grade: {}\".format(grades.mean()))\r\n",
        "bins = np.arange(-0.1, 3.1, 0.1)\r\n",
        "print(\"bins: {}\".format(bins))\r\n",
        "\r\n",
        "grade_counts = grades.value_counts(bins=bins, sort=False)\r\n",
        "grade_counts.plot(figsize=(10,8), title=\"Grade Counts\", kind=\"bar\", xlabel=\"Grade\", ylabel=\"Counts\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gphEqSiVOiij"
      },
      "source": [
        "### Word/Character Count/Length Distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLaaREEiPlch"
      },
      "source": [
        "# Create new grade bins.\r\n",
        "category_bins = np.arange(-0.1, 3.5, 0.5)\r\n",
        "train_df['gradeCategory'] = pd.cut(train_df['meanGrade'], bins=category_bins, labels=[\"<0.5\", \"<1.0\", \"<1.5\", \"<2.0\", \"<2.5\", \"<3.0\", \"3.0\"])\r\n",
        "\r\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,6))\r\n",
        "train_df.groupby(['meanGrade']).mean()['avgWordLength'].plot(title=\"Average Sentence Word Length by Grade\", ylabel=\"avgWordLength\", ax=axes[0])\r\n",
        "train_df.groupby(['gradeCategory']).mean()['avgWordLength'].plot(title=\"Average Sentence Word Length by Grade\", ylabel=\"avgWordLength\", ax=axes[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7sPX8chROrH"
      },
      "source": [
        "### Stop word Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M1DvFI_QILa"
      },
      "source": [
        "train_df.groupby(['meanGrade']).mean()['numStopWords'].plot(title=\"Average numStopWords by Grade\", ylabel=\"numStopWords\", ax=axes[0])\r\n",
        "train_df.groupby(['gradeCategory']).mean()['numStopWords'].plot(title=\"Average numStopWords by Grade\", ylabel=\"numStopWords\", ax=axes[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JpB_aAbRewo"
      },
      "source": [
        "### Word type analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjZGCwIvRind"
      },
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,6))\r\n",
        "train_df.groupby(['meanGrade']).mean()['numHashtags'].plot(title=\"Average numHashtags by Grade\", ylabel=\"numHashtags\", ax=axes[0])\r\n",
        "train_df.groupby(['gradeCategory']).mean()['numHashtags'].plot(title=\"Average numHashtags by Grade\", ylabel=\"numHashtags\", ax=axes[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5soMnTe2Rmnw"
      },
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,6))\r\n",
        "train_df.groupby(['meanGrade']).mean()['numDigits'].plot(title=\"Average numDigits by Grade\", ylabel=\"numDigits\", ax=axes[0])\r\n",
        "train_df.groupby(['gradeCategory']).mean()['numDigits'].plot(title=\"Average numDigits by Grade\", ylabel=\"numDigits\", ax=axes[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pz4zo3q3R37R"
      },
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,6))\r\n",
        "train_df.groupby(['meanGrade']).mean()['numUpperCase'].plot(title=\"Average numUpperCase by Grade\", ylabel=\"numUpperCase\", ax=axes[0])\r\n",
        "train_df.groupby(['gradeCategory']).mean()['numUpperCase'].plot(title=\"Average numUpperCase by Grade\", ylabel=\"numUpperCase\", ax=axes[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq3ZjcofRhhr"
      },
      "source": [
        "## Text Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ub9ZBXuSSfYQ"
      },
      "source": [
        "# Get set of replacement words.\r\n",
        "replacement_words = train_df['edit']\r\n",
        "print(replacement_words.value_counts())\r\n",
        "print(\"Num replaced words == 'trump': {}\".format(len(train_df[train_df['edit'] == 'trump'])))\r\n",
        "print(\"Mean grade of replaced words == 'party': {}\".format(train_df[train_df['edit'] == 'party'].mean()['meanGrade']))\r\n",
        "print(\"Mean grade of replaced words == 'hair': {}\".format(train_df[train_df['edit'] == 'hair'].mean()['meanGrade']))\r\n",
        "print(\"Mean grade of replaced words == 'dance': {}\".format(train_df[train_df['edit'] == 'dance'].mean()['meanGrade']))\r\n",
        "print(\"Mean grade of replaced words == 'dancing': {}\".format(train_df[train_df['edit'] == 'dancing'].mean()['meanGrade']))\r\n",
        "print(\"Mean grade of replaced words == 'circus': {}\".format(train_df[train_df['edit'] == 'circus'].mean()['meanGrade']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytTzvgDaSgtO"
      },
      "source": [
        "# Funniest sentence.\r\n",
        "train_df.loc[train_df['meanGrade'].idxmax()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj3R2MbmSku2"
      },
      "source": [
        "# Find common words.\r\n",
        "freq = pd.Series(' '.join(processed['editedSentence']).split()).value_counts()\r\n",
        "top_freq = freq[:10]\r\n",
        "top_freq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhR_xF2nSl4r"
      },
      "source": [
        "# Find rare words.\r\n",
        "bottom_freq = freq[-10:]\r\n",
        "bottom_freq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnnIpvjWSrV5"
      },
      "source": [
        "# We cannot delete the most/least common words because they are the edited words.\r\n",
        "print(\"Least common words that are replacement words: {}\".format(set.intersection(set(bottom_freq.index.tolist()), set(replacement_words.values.tolist()))))\r\n",
        "print(\"Most common words that are replacement words: {}\".format(set.intersection(set(top_freq.index.tolist()), set(replacement_words.values.tolist()))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCR6a2SeZHIh"
      },
      "source": [
        "# Plot the feature correlation\r\n",
        "import seaborn as sns\r\n",
        "corr = numeric_data.corr()\r\n",
        "f, ax = plt.subplots(figsize=(11, 9))\r\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)\r\n",
        "sns.heatmap(corr, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}